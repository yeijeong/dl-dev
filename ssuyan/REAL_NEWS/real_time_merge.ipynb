{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4.element\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "conn = pymysql.connect(\n",
    "    host='34.64.240.96'\n",
    "    , user='root'\n",
    "    , password='tndusWkd1.'\n",
    "    , db='final_project'\n",
    "    , charset='utf8'\n",
    ")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 후 행 개수:  0\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.DataFrame(\n",
    "    data=None,\n",
    "    index=None,\n",
    "    columns=['date','title','link','content']\n",
    ")\n",
    "\n",
    "#중복 행 지우기\n",
    "news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "print(\"중복 제거 후 행 개수: \",len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConnectionError방지\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "\n",
    "# BeautifulSoup 객체 생성\n",
    "def get_soup_obj(url):\n",
    "    res = requests.get(url, headers = headers, verify=False)\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스의 기본 정보 가져오기\n",
    "def get_top3_news_info():\n",
    "    news_urls =[]\n",
    "    \n",
    "    # 해당 분야 상위 뉴스 목록 주소\n",
    "    sec_url = \"https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=101\"\n",
    "    \n",
    "    # 해당 분야 상위 뉴스 HTML 가져오기\n",
    "    soup = get_soup_obj(sec_url)\n",
    "  \n",
    "    # 해당 분야 상위 뉴스 3개 가져오기\n",
    "\n",
    "    lis3 = soup.find('ul', class_='type06_headline').find_all(\"li\", limit=3)\n",
    "    for li in lis3:\n",
    "        news_url = li.a.attrs.get('href')\n",
    "        news_urls.append(news_url)\n",
    "\n",
    "    \n",
    "    print(news_urls)\n",
    "    return news_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 3개 뉴스 크롤링\n",
    "def F_crawling(news_urls):\n",
    "    news_titles=[]\n",
    "    news_contents=[]\n",
    "    news_dates=[]\n",
    "\n",
    "    for url in news_urls:\n",
    "        news_html = get_soup_obj(url)\n",
    "\n",
    "\n",
    "        # 뉴스 제목 가져오기\n",
    "        title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "        if title == None:\n",
    "            title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "\n",
    " \n",
    "        # 뉴스 본문 가져오기\n",
    "        content = news_html.select(\"div#dic_area\")\n",
    "        if content == []:\n",
    "            content = news_html.select(\"#articeBody\")\n",
    "            \n",
    "        # 기사 텍스트만 가져오기\n",
    "        # list합치기\n",
    "        content = ''.join(str(content))\n",
    "\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        pattern1 = '<[^>]*>'\n",
    "        title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "        content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = content.replace(pattern2, '')\n",
    "\n",
    "        news_titles.append(title)\n",
    "        news_contents.append(content)\n",
    "\n",
    "        html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "        news_date = html_date.attrs['data-date-time']\n",
    "        news_dates.append(news_date)\n",
    "\n",
    "    a = pd.DataFrame({'date':news_dates, 'title':news_titles, 'link':news_urls, 'content':news_contents})\n",
    "\n",
    "    #중복 행 지우기\n",
    "    a = a.drop_duplicates(keep='first',ignore_index=True)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clearnews():\n",
    "#     special_char = ' \\/:*?\\\"\\\"<>|\\'\\' '\n",
    "#     for clear in special_char:\n",
    "#         if clear in title:\n",
    "#             print(title.find(clear),clear)\n",
    "#             title = title.replace(clear,'')\n",
    "#     print(title)\n",
    "\n",
    "# def clearnews():\n",
    "#     special_char = ' \\/:*?\\\"\\\"<>|\\'\\' '\n",
    "#     for clear in special_char:\n",
    "#         if clear in content:\n",
    "#             print(content.find(clear),clear)\n",
    "#             content = content.replace(clear,'')\n",
    "#     print(content)\n",
    "\n",
    "def text_clean(text):\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[a-zA-Z0-9]'    # 숫자와 알파벳 제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[\\n]'            # \\n제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[\\t]'            # \\n제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://n.news.naver.com/mnews/article/003/0011958077?sid=101', 'https://n.news.naver.com/mnews/article/032/0003234469?sid=101', 'https://n.news.naver.com/mnews/article/003/0011958075?sid=101']\n",
      "                  date                              title   \n",
      "0  2023-07-06 15:26:12  \"그때 그시절\" 17년전 산돌 폰트 만난다…'SD픽셀' 출시  \\\n",
      "1  2023-07-06 15:26:08          예적금 보호 안내문 붙은 새마을금고[경향포토]   \n",
      "2  2023-07-06 15:26:06     NH농협은행, 개인사업자 저금리 '지자체협약대출' 출시   \n",
      "\n",
      "                                                link   \n",
      "0  https://n.news.naver.com/mnews/article/003/001...  \\\n",
      "1  https://n.news.naver.com/mnews/article/032/000...   \n",
      "2  https://n.news.naver.com/mnews/article/003/001...   \n",
      "\n",
      "                                             content  \n",
      "0  [\\n당시 복원해 픽셀 폰트로 재탄생세기말 감성의 손글씨 느낌 10종\\n\\n\\n\\n...  \n",
      "1  [\\n\\n\\n\\n\\n새마을금고 건전성에 대한 우려로 정부가 '범정부 대응단'을 구성...  \n",
      "2  [\\n\\n\\n\\n\\n[서울=뉴시스]이주혜 기자 = NH농협은행은 지방자치단체와 협약...  \n"
     ]
    }
   ],
   "source": [
    "# while True:\n",
    "news_urls = get_top3_news_info()\n",
    "a = F_crawling(news_urls)\n",
    "news_df=pd.concat([news_df,a])\n",
    "news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "print(news_df.head())\n",
    "\n",
    "# time.sleep(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewsDate = news_df['date'].to_list()\n",
    "NewsTItle = news_df['title'].to_list()\n",
    "NewsUrl = news_df['link'].to_list()\n",
    "NewsContent = news_df['content'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO real_news (NewsDates,NewsTitles,NewsUrls,NewsContents) VALUES ('2023-07-06 15:26:12', '\"그때 그시절\" 17년전 산돌 폰트 만난다…'SD픽셀' 출시', 'https://n.news.naver.com/mnews/article/003/0011958077?sid=101', '[\n",
      "당시 복원해 픽셀 폰트로 재탄생세기말 감성의 손글씨 느낌 10종\n",
      "\n",
      "\n",
      "\n",
      "[서울=뉴시스] 산돌 'SD 픽셀' 시리즈. (이미지=산돌 제공) 2023.07.06. photo@newsis.com[서울=뉴시스] 배민욱 기자 = 크리에이터 콘텐츠 플랫폼 산돌은 17년 전 인기를 끌었던 자사 픽셀 폰트를 재해석해 현대화한 'SD 픽셀' 시리즈 10종을 선보인다고 6일 밝혔다.   SD 픽셀 시리즈는 2000년대 초반 웹폰트로 처음 공개됐다. 2006년 싸이월드 등에서 폰트 판매 서비스를 시작하는 동시에 자신의 개성을 표현할 수 있는 수단으로 이용됐다. 산돌은 픽셀 폰트 시리즈를 최근 인기인 Y2K 트렌드에 맞게 복원하고 가공해 새롭게 선보였다. SD 픽셀쪽지,  SD 픽셀비밀쪽지, SD 픽셀굿모닝, SD 픽셀굿밤, SD 픽셀동동동, SD 픽셀쁘띠공주, SD 픽셀사과나무, SD픽셀섬섬, SD픽셀요술지팡이, SD픽셀종이학까지 세기말 감성이 담긴 귀여운 손글씨 느낌의 폰트가 총 10종으로 출시됐다. 특히 SD 픽셀 시리즈 중 SD 픽셀쪽지, SD 픽셀비밀쪽지는 모바일에서 사용할 수 있도록 선보였다. iOS 사용자의 경우 산돌구름 모바일 앱에서, 안드로이드 사용자의 경우 갤럭시 스토어를 통해 구매할 수 있다. 사용자는 카카오톡 등의 메신저, 그 외 모바일 서비스에서 폰트를 적용해 이용이 가능하다. 산돌은 SD 픽셀 시리즈 출시를 맞아 세기말 감성의 디지털 굿즈 '카카오톡 테마'와 'Y2K 인스타그램 필터'를 함께 제작해 무료로 제공한다. 카카오톡 테마는 산돌구름의 블로그에서 다운로드 받아 사용할 수 있다. 인스타그램 필터는 산돌의 인스타그램 계정에서 배포될 예정이다.\n",
      "]')\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "(1064, \"You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'SD픽셀' 출시', 'https://n.news.naver.com/mnews/article/003/0011958077?sid...' at line 1\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m     sql \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mINSERT INTO real_news (NewsDates,NewsTitles,NewsUrls,NewsContents) VALUES (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mdate\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mtitle\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39murl\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mcontent\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(sql)\n\u001b[1;32m----> 4\u001b[0m     cur\u001b[39m.\u001b[39;49mexecute(sql)\n\u001b[0;32m      5\u001b[0m conn\u001b[39m.\u001b[39mcommit()\n\u001b[0;32m      6\u001b[0m conn\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\green\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pymysql\\cursors.py:153\u001b[0m, in \u001b[0;36mCursor.execute\u001b[1;34m(self, query, args)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    151\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmogrify(query, args)\n\u001b[1;32m--> 153\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(query)\n\u001b[0;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executed \u001b[39m=\u001b[39m query\n\u001b[0;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\green\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pymysql\\cursors.py:322\u001b[0m, in \u001b[0;36mCursor._query\u001b[1;34m(self, q)\u001b[0m\n\u001b[0;32m    320\u001b[0m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_db()\n\u001b[0;32m    321\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_result()\n\u001b[1;32m--> 322\u001b[0m conn\u001b[39m.\u001b[39;49mquery(q)\n\u001b[0;32m    323\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_get_result()\n\u001b[0;32m    324\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrowcount\n",
      "File \u001b[1;32mc:\\Users\\green\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pymysql\\connections.py:558\u001b[0m, in \u001b[0;36mConnection.query\u001b[1;34m(self, sql, unbuffered)\u001b[0m\n\u001b[0;32m    556\u001b[0m     sql \u001b[39m=\u001b[39m sql\u001b[39m.\u001b[39mencode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, \u001b[39m\"\u001b[39m\u001b[39msurrogateescape\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    557\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_command(COMMAND\u001b[39m.\u001b[39mCOM_QUERY, sql)\n\u001b[1;32m--> 558\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_affected_rows \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_query_result(unbuffered\u001b[39m=\u001b[39;49munbuffered)\n\u001b[0;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_affected_rows\n",
      "File \u001b[1;32mc:\\Users\\green\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pymysql\\connections.py:822\u001b[0m, in \u001b[0;36mConnection._read_query_result\u001b[1;34m(self, unbuffered)\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    821\u001b[0m     result \u001b[39m=\u001b[39m MySQLResult(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 822\u001b[0m     result\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m    823\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m result\n\u001b[0;32m    824\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mserver_status \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\green\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pymysql\\connections.py:1200\u001b[0m, in \u001b[0;36mMySQLResult.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1200\u001b[0m         first_packet \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnection\u001b[39m.\u001b[39;49m_read_packet()\n\u001b[0;32m   1202\u001b[0m         \u001b[39mif\u001b[39;00m first_packet\u001b[39m.\u001b[39mis_ok_packet():\n\u001b[0;32m   1203\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_ok_packet(first_packet)\n",
      "File \u001b[1;32mc:\\Users\\green\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pymysql\\connections.py:772\u001b[0m, in \u001b[0;36mConnection._read_packet\u001b[1;34m(self, packet_type)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39munbuffered_active \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    771\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39munbuffered_active \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 772\u001b[0m     packet\u001b[39m.\u001b[39;49mraise_for_error()\n\u001b[0;32m    773\u001b[0m \u001b[39mreturn\u001b[39;00m packet\n",
      "File \u001b[1;32mc:\\Users\\green\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pymysql\\protocol.py:221\u001b[0m, in \u001b[0;36mMysqlPacket.raise_for_error\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39mif\u001b[39;00m DEBUG:\n\u001b[0;32m    220\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39merrno =\u001b[39m\u001b[39m\"\u001b[39m, errno)\n\u001b[1;32m--> 221\u001b[0m err\u001b[39m.\u001b[39;49mraise_mysql_exception(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data)\n",
      "File \u001b[1;32mc:\\Users\\green\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pymysql\\err.py:143\u001b[0m, in \u001b[0;36mraise_mysql_exception\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m errorclass \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m     errorclass \u001b[39m=\u001b[39m InternalError \u001b[39mif\u001b[39;00m errno \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m \u001b[39melse\u001b[39;00m OperationalError\n\u001b[1;32m--> 143\u001b[0m \u001b[39mraise\u001b[39;00m errorclass(errno, errval)\n",
      "\u001b[1;31mProgrammingError\u001b[0m: (1064, \"You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'SD픽셀' 출시', 'https://n.news.naver.com/mnews/article/003/0011958077?sid...' at line 1\")"
     ]
    }
   ],
   "source": [
    "for date,title,url,content in zip(NewsDate, NewsTItle, NewsUrl, NewsContent):\n",
    "    sql = \"INSERT INTO real_news (NewsDates,NewsTitles,NewsUrls,NewsContents) VALUES (%s, %s, %s, %s)\" % (\"'\"+date+\"'\", \"'\"+title+\"'\", \"'\"+url+\"'\", \"'\"+content+\"'\")\n",
    "    print(sql)\n",
    "    cur.execute(sql)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###데이터 프레임으로 만들기###\n",
    "# from datetime import date, timedelta\n",
    "# import pandas as pd\n",
    "\n",
    "# #데이터 프레임 만들기\n",
    "\n",
    "# # list=[]\n",
    "# a = pd.DataFrame({'date':news_dates,'title':news_titles,'link':news_urls,'content':news_contents})\n",
    "\n",
    "# # list.append([news_df,a])\n",
    "\n",
    "# #데이터 프레임 저장\n",
    "# now = datetime.now() \n",
    "# a.to_csv('C:\\\\big16\\\\trading_project\\\\ssuyan\\\\REAL_NEWS\\\\_{}.csv'.format(now.strftime('%Y%m%d_%H%M%S')),encoding='utf-8-sig',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
