{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4.element\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame(\n",
    "    index=None,\n",
    "    columns={'date':news_dates,'title':news_titles,'link':news_urls,'content':news_contents}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConnectionError방지\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "\n",
    "# BeautifulSoup 객체 생성\n",
    "def get_soup_obj(url):\n",
    "    res = requests.get(url, headers = headers, verify=False)\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스의 기본 정보 가져오기\n",
    "def get_top3_news_info():\n",
    "    news_urls =[]\n",
    "    \n",
    "    # 해당 분야 상위 뉴스 목록 주소\n",
    "    sec_url = \"https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=101\"\n",
    "    \n",
    "    # 해당 분야 상위 뉴스 HTML 가져오기\n",
    "    soup = get_soup_obj(sec_url)\n",
    "  \n",
    "    # 해당 분야 상위 뉴스 3개 가져오기\n",
    "\n",
    "    lis3 = soup.find('ul', class_='type06_headline').find_all(\"li\", limit=3)\n",
    "    for li in lis3:\n",
    "        news_url = li.a.attrs.get('href')\n",
    "        news_urls.append(news_url)\n",
    "    \n",
    "    print(news_urls)\n",
    "    return news_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://n.news.naver.com/mnews/article/028/0002646939?sid=101', 'https://n.news.naver.com/mnews/article/031/0000756963?sid=101', 'https://n.news.naver.com/mnews/article/144/0000898251?sid=101']\n"
     ]
    }
   ],
   "source": [
    "# 상위 3개 뉴스 크롤링\n",
    "news_urls = get_top3_news_info()\n",
    "\n",
    "for url in news_urls:\n",
    "    news_html = get_soup_obj(url)\n",
    "\n",
    "\n",
    "    # 뉴스 제목 가져오기\n",
    "    title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    if title == None:\n",
    "        title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "\n",
    "    # 뉴스 본문 가져오기\n",
    "    content = news_html.select(\"div#dic_area\")\n",
    "    if content == []:\n",
    "        content = news_html.select(\"#articeBody\")\n",
    "        \n",
    "    # 기사 텍스트만 가져오기\n",
    "    # list합치기\n",
    "    content = ''.join(str(content))\n",
    "\n",
    "    # html태그제거 및 텍스트 다듬기\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "    content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "    pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "    content = content.replace(pattern2, '')\n",
    "\n",
    "    news_titles.append(title)\n",
    "    news_contents.append(content)\n",
    "\n",
    "    html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "    news_date = html_date.attrs['data-date-time']\n",
    "    news_dates.append(news_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 후 행 개수:  3\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'date':news_dates,'title':news_titles,'link':news_urls,'content':news_contents})\n",
    "\n",
    "#중복 행 지우기\n",
    "news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "print(\"중복 제거 후 행 개수: \",len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_df() :\n",
    "    a = pd.DataFrame({'date':news_dates,'title':news_titles,'link':news_urls,'content':news_contents})\n",
    "    \n",
    "    #중복 행 지우기\n",
    "    a = a.drop_duplicates(keep='first',ignore_index=True)\n",
    "    print(\"중복 제거 후 행 개수: \",len(a))\n",
    "\n",
    "    pd.concat([news_df, a])\n",
    "\n",
    "    # #중복 행 지우기\n",
    "    # news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "    # print(\"중복 제거 후 행 개수: \",len(news_df))\n",
    "\n",
    "    print(news_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://n.news.naver.com/mnews/article/016/0002165895?sid=101', 'https://n.news.naver.com/mnews/article/015/0004864938?sid=101', 'https://n.news.naver.com/mnews/article/648/0000017710?sid=101']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     news_date \u001b[39m=\u001b[39m html_date\u001b[39m.\u001b[39mattrs[\u001b[39m'\u001b[39m\u001b[39mdata-date-time\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     38\u001b[0m     news_dates\u001b[39m.\u001b[39mappend(news_date)\n\u001b[1;32m---> 40\u001b[0m add_df()\n\u001b[0;32m     42\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m30\u001b[39m)\n\u001b[0;32m     44\u001b[0m t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m, in \u001b[0;36madd_df\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_df\u001b[39m() :\n\u001b[1;32m----> 2\u001b[0m     a \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mdate\u001b[39;49m\u001b[39m'\u001b[39;49m:news_dates,\u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m:news_titles,\u001b[39m'\u001b[39;49m\u001b[39mlink\u001b[39;49m\u001b[39m'\u001b[39;49m:news_urls,\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m:news_contents})\n\u001b[0;32m      4\u001b[0m     \u001b[39m#중복 행 지우기\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mdrop_duplicates(keep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m,ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\green\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\core\\frame.py:708\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    702\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    703\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    704\u001b[0m     )\n\u001b[0;32m    706\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    707\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    709\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    710\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\green\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\core\\internals\\construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    479\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\green\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\core\\internals\\construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    113\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    116\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\green\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\core\\internals\\construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    653\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[0;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    657\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[0;32m    658\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    659\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "contents =[]\n",
    "dates = []\n",
    "\n",
    "t=1\n",
    "while t<5 :\n",
    "    news_urls = get_top3_news_info()\n",
    "\n",
    "    for url in news_urls:\n",
    "        news_html = get_soup_obj(url)\n",
    "\n",
    "        # 뉴스 제목 가져오기\n",
    "        title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "        if title == None:\n",
    "            title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "\n",
    "        # 뉴스 본문 가져오기\n",
    "        content = news_html.select(\"div#dic_area\")\n",
    "        if content == []:\n",
    "            content = news_html.select(\"#articeBody\")\n",
    "            \n",
    "        # 기사 텍스트만 가져오기\n",
    "        # list합치기\n",
    "        content = ''.join(str(content))\n",
    "\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        pattern1 = '<[^>]*>'\n",
    "        title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "        content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = content.replace(pattern2, '')\n",
    "\n",
    "        news_titles.append(title)\n",
    "        news_contents.append(content)\n",
    "\n",
    "        html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "        news_date = html_date.attrs['data-date-time']\n",
    "        news_dates.append(news_date)\n",
    "    \n",
    "    add_df()\n",
    "\n",
    "    time.sleep(30)\n",
    "\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-06 00:16:01</td>\n",
       "      <td>박원순 시장 때 멈춘 서울 재건축 재개발 사업 재개</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/021/000...</td>\n",
       "      <td>[\\n\\n\\n\\n\\n서울시 달동네 모습. 게티이미지뱅크  오세훈 시장 핵심사업 ‘신...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-06 00:14:01</td>\n",
       "      <td>파이퍼샌들러,\"코인베이스 중립 하향\"</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "      <td>[\\n암호자산 규제 방안 불확실하고 거래량 지속 줄어\"2년래 최저 거래량과 사용자 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-06 00:07:43</td>\n",
       "      <td>남성현 산림청장 \"강원세계산림엑스포 적극 지원\"</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/654/000...</td>\n",
       "      <td>[\\n춘천 방문서 성공 개최 협력 약속\\n\\n\\n\\n▲ 춘천목재산업단지 준공식이 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                         title   \n",
       "0  2023-07-06 00:16:01  박원순 시장 때 멈춘 서울 재건축 재개발 사업 재개  \\\n",
       "1  2023-07-06 00:14:01          파이퍼샌들러,\"코인베이스 중립 하향\"   \n",
       "2  2023-07-06 00:07:43    남성현 산림청장 \"강원세계산림엑스포 적극 지원\"   \n",
       "\n",
       "                                                link   \n",
       "0  https://n.news.naver.com/mnews/article/021/000...  \\\n",
       "1  https://n.news.naver.com/mnews/article/015/000...   \n",
       "2  https://n.news.naver.com/mnews/article/654/000...   \n",
       "\n",
       "                                             content  \n",
       "0  [\\n\\n\\n\\n\\n서울시 달동네 모습. 게티이미지뱅크  오세훈 시장 핵심사업 ‘신...  \n",
       "1  [\\n암호자산 규제 방안 불확실하고 거래량 지속 줄어\"2년래 최저 거래량과 사용자 ...  \n",
       "2  [\\n춘천 방문서 성공 개최 협력 약속\\n\\n\\n\\n▲ 춘천목재산업단지 준공식이 5...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###데이터 프레임으로 만들기###\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "#데이터 프레임 만들기\n",
    "\n",
    "# list=[]\n",
    "a = pd.DataFrame({'date':news_dates,'title':news_titles,'link':news_urls,'content':news_contents})\n",
    "\n",
    "# list.append([news_df,a])\n",
    "\n",
    "#데이터 프레임 저장\n",
    "now = datetime.now() \n",
    "a.to_csv('C:\\\\big16\\\\trading_project\\\\ssuyan\\\\REAL_NEWS\\\\_{}.csv'.format(now.strftime('%Y%m%d_%H%M%S')),encoding='utf-8-sig',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
