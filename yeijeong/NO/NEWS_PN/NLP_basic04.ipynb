{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_basic04.ipynb","provenance":[{"file_id":"https://gist.github.com/Lucia-KIM/6013cc738166d2c0b8ec66688eeb30a6#file-nlp_basic04-ipynb","timestamp":1689146882127}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"0qzRE4LahrJB"},"source":["from keras.models import Sequential\n","from keras.layers import SimpleRNN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lVcZbRBjh-Pq","outputId":"639fc280-fb02-4832-a1e2-5fd7546950cf"},"source":["model = Sequential()\n","model.add(SimpleRNN(3, input_shape=(2,10)))\n","# model.add(SimpleRNN(hidden_size, input_shape=(timesteps, input_dim)))\n","# model.add(SimpleRNN(hidden_size, input_length=M, input_dim=N))\n","\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","simple_rnn (SimpleRNN)       (None, 3)                 42        \n","=================================================================\n","Total params: 42\n","Trainable params: 42\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m1HsWxW5iK1a"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZCbUe3ei1c4I"},"source":["# 10. 워드 임베딩(Word Embedding)\n","### 1) 영어 Word2Vec 만들기"]},{"cell_type":"code","metadata":{"id":"DnP2hjHJATi0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"97a9a03d-1c22-425f-ec44-37724b03d981"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"b2ZpFSRV1rl6"},"source":["import urllib.request\n","import zipfile\n","from lxml import etree\n","import re\n","from nltk.tokenize import word_tokenize, sent_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fsAsNCho16Td","outputId":"817d6569-0ded-4119-f605-68112091a693"},"source":["# 데이터 다운로드\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('ted_en-20160408.xml', <http.client.HTTPMessage at 0x7f0c4ec1c510>)"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"oUpWBiA72HFH"},"source":["# 훈련 데이터 전처리\n","targetXML = open('ted_en-20160408.xml', 'r', encoding='utf-8')\n","target_text = etree.parse(targetXML)\n","\n","parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n","\n","content_text =re.sub(r'\\([^)]*\\)', '', parse_text)\n","sent_text = sent_tokenize(content_text)\n","\n","normalized_text = []\n","for string in sent_text:\n","    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n","    normalized_text.append(tokens)\n","\n","result = [word_tokenize(sentence) for sentence in normalized_text]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AxPazUv2qut","outputId":"32839d9c-91e6-4c3c-f52c-72ef81325002"},"source":["print('총 샘플 수 : {}'.format(len(result)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["총 샘플 수 : 273424\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8JJrK3C9HS5","outputId":"137ee8bf-09b1-4ed8-9576-ff2c6cbb2a37"},"source":["for line in result[:3]:\n","    print(line)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n","['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n","['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"airRCcFZ9W3y"},"source":["### - Word2Vec 훈련시키기"]},{"cell_type":"code","metadata":{"id":"Fy0Z4yxW9R98"},"source":["from gensim.models import Word2Vec\n","model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)\n","\n","# size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n","# window = 컨텍스트 윈도우 크기\n","# min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n","# workers = 학습을 위한 프로세스 수\n","# sg = 0은 CBOW, 1은 Skip-gram."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y6vAyOvU96_Y","outputId":"9f07067d-9512-456c-b510-16693caf6a65"},"source":["# Word2Vec는 입력한 단어에 대해서 가장 유사한 단어들을 출력하는 model.wv.most_similar을 지원한다.\n","\n","model_result = model.wv.most_similar('man')\n","print(model_result)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('woman', 0.8441935181617737), ('guy', 0.8224888443946838), ('boy', 0.7669707536697388), ('girl', 0.7566667199134827), ('lady', 0.7472554445266724), ('gentleman', 0.7385295033454895), ('poet', 0.7173836827278137), ('kid', 0.7116258144378662), ('soldier', 0.6919503211975098), ('friend', 0.6820288896560669)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qT1GUZYw-MZd"},"source":["from gensim.models import KeyedVectors\n","model.wv.save_word2vec_format('eng_w2v') #모델 저장\n","\n","# loaded_model = KeyedVectors.load_word2vec_format('eng_w2v') # 저장된 모델 불러오기"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xcp0rm1VY8ux","outputId":"f7a2e6a5-2fd4-431d-f5d9-f06179f6658f"},"source":["!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-04-08 07:30:44,857 - word2vec2tensor - INFO - running /usr/local/lib/python3.7/dist-packages/gensim/scripts/word2vec2tensor.py --input eng_w2v --output eng_w2v\n","2021-04-08 07:30:44,858 - utils_any2vec - INFO - loading projection weights from eng_w2v\n","2021-04-08 07:30:47,147 - utils_any2vec - INFO - loaded (21613, 100) matrix from eng_w2v\n","2021-04-08 07:30:49,145 - word2vec2tensor - INFO - 2D tensor file saved to eng_w2v_tensor.tsv\n","2021-04-08 07:30:49,145 - word2vec2tensor - INFO - Tensor metadata file saved to eng_w2v_metadata.tsv\n","2021-04-08 07:30:49,148 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gXRQQjTY_Yjt"},"source":["### 2) 한국어 Word2Vec 만들기(네이버 영화 리뷰)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUt8EwK_ALYd","outputId":"5067d7cf-f452-4b8d-ad92-2578e755a16b"},"source":["pip install konlpy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting konlpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n","\u001b[K     |████████████████████████████████| 19.4MB 1.6MB/s \n","\u001b[?25hCollecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Collecting JPype1>=0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n","\u001b[K     |████████████████████████████████| 460kB 31.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n","Collecting beautifulsoup4==4.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n","\u001b[K     |████████████████████████████████| 92kB 6.0MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n","Installing collected packages: colorama, JPype1, beautifulsoup4, konlpy\n","  Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VlPWsosk_Ja_"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import urllib.request\n","from gensim.models.word2vec import Word2Vec\n","from konlpy.tag import Okt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0zYb7KLAJXj","outputId":"2b48e5e6-5d3e-4bcb-f4b9-6a4f0a5256bd"},"source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('ratings.txt', <http.client.HTTPMessage at 0x7f999d52f150>)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"_vgxzlsOAeFr","outputId":"dd47a4bf-35ca-4e54-cd00-429333807174"},"source":["train_data = pd.read_table('ratings.txt')\n","train_data[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>8112052</td>\n","      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8132799</td>\n","      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4655635</td>\n","      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9251303</td>\n","      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10067386</td>\n","      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id                                           document  label\n","0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n","1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n","2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n","3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n","4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ouhMn_--A-dx","outputId":"885826e2-eafd-4934-8c47-e138bcffb0f0"},"source":["print(len(train_data))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["200000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5sBQ5BHzBOAH","outputId":"3079835c-b8bd-40b8-cea4-c575b58ae475"},"source":["print(train_data.isnull().values.any()) # NULL 값 존재 유무"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QSHdkbzuBTYd","outputId":"fce6d74f-d18b-48a0-cea7-bd1fd98a3b66"},"source":["train_data = train_data.dropna(how='any') # Null 값이 존재하는 행 제거\n","print(train_data.isnull().values.any())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDJoBOdcBj0N","outputId":"518563ba-0bf2-4307-8836-9705e0bfe189"},"source":["print(len(train_data))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["199992\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QSEOBMZOBmvl"},"source":["# 정규 표현식을 통한 한글 외 문자 제거\n","\n","train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"L0eMmAxXB59d","outputId":"a24879ea-1611-416b-9d12-479c1da23d99"},"source":["train_data[:5] #띄어쓰기가 사라짐..."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>8112052</td>\n","      <td>어릴때보고지금다시봐도재밌어요ㅋㅋ</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8132799</td>\n","      <td>디자인을배우는학생으로외국디자이너와그들이일군전통을통해발전해가는문화산업이부러웠는데사실우...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4655635</td>\n","      <td>폴리스스토리시리즈는부터뉴까지버릴께하나도없음최고</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9251303</td>\n","      <td>와연기가진짜개쩔구나지루할거라고생각했는데몰입해서봤다그래이런게진짜영화지</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10067386</td>\n","      <td>안개자욱한밤하늘에떠있는초승달같은영화</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id                                           document  label\n","0   8112052                                  어릴때보고지금다시봐도재밌어요ㅋㅋ      1\n","1   8132799  디자인을배우는학생으로외국디자이너와그들이일군전통을통해발전해가는문화산업이부러웠는데사실우...      1\n","2   4655635                          폴리스스토리시리즈는부터뉴까지버릴께하나도없음최고      1\n","3   9251303              와연기가진짜개쩔구나지루할거라고생각했는데몰입해서봤다그래이런게진짜영화지      1\n","4  10067386                                안개자욱한밤하늘에떠있는초승달같은영화      1"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"3ozCC8ifDKKB"},"source":["# 불용어 정의\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6aKuzzgoB9Cw"},"source":["okt =Okt()\n","tokenized_data = []\n","for sentence in train_data['document']:\n","    temp_X = okt.morphs(sentence, stem=True)\n","    temp_X = [word for word in temp_X if not word in stopwords]\n","    tokenized_data.append(temp_X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"oCAOvD2PCy7r","outputId":"99a7e88d-7fe4-43f3-9e9b-d3e5afe80885"},"source":["print('리뷰 최대 길이: ', max(len(l) for l in tokenized_data))\n","print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n","plt.hist([len(s) for s in tokenized_data], bins = 50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["리뷰 최대 길이:  68\n","리뷰의 평균 길이 : 10.669446777871116\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"oOPvMBIbEjz-"},"source":["from gensim.models import Word2Vec\n","model = Word2Vec(sentences = tokenized_data, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7a8o7S8TLvZ","outputId":"9d116dfd-194f-4894-feed-c6062bb6806d"},"source":["# 완성된 임베딩 매트릭스의 크기 확인\n","model.wv.vectors.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(17947, 100)"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vONAoTGMTN3e","outputId":"3f31d0a7-7c1c-45df-954a-7648e2d9c306"},"source":["print(model.wv.most_similar(\"최민식\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('양동근', 0.8718291521072388), ('안성기', 0.8718253374099731), ('정재영', 0.8522905111312866), ('채민서', 0.8490892648696899), ('단역', 0.8430312871932983), ('윤제문', 0.8410438895225525), ('류덕환', 0.8403877019882202), ('한석규', 0.8399507999420166), ('서영희', 0.8391263484954834), ('김승우', 0.8350732922554016)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JzXDVx7O1dxm"},"source":["### 3) 케라스 임베딩 층(Keras Embedding layer) 불러와 사용하기"]},{"cell_type":"code","metadata":{"id":"RgPIp1g5cNcf"},"source":["from keras.layers import Embedding\n","# 케라스의 임베딩 층 구현\n","v = Embedding(20000, 128, input_length=500)\n","# vocab_size : 텍스트 데이터의 전체 단어 집합의 크기\n","# output_dim : 워드 임베딩 후의 임베딩 벡터의 차원\n","# input_length : 입력 시퀀스의 길이, 만약 갖고있는 각 샘플의 길이가 500개의 단어로 구성되어있다면 이 값은 500"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYratF8p4-xF","outputId":"08765d52-d4b6-4ae2-8d7a-e3568420766f"},"source":["v"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.layers.embeddings.Embedding at 0x7f0ca9a30210>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"_JG5UFCS6PkH"},"source":["#문장의 긍, 부정을 판단하는 감성 분류 모델을 만들어 보자.\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n","y_train = [1, 0, 0, 1, 1, 0, 1] #긍정인 문장은 레이블 1, 부정인 문장은 레이블이 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mdjxhm3E9agH","outputId":"f53e5644-55bd-4733-f14a-31adcfbc9a4e"},"source":["t = Tokenizer()\n","t.fit_on_texts(sentences) #케라스의 Tokenizer()를 사용하여 토큰화 진행\n","vocab_size = len(t.word_index)+1\n","# t.word_index : 각 단어에 대한 정수값 부여(1~15)\n","# 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만,\n","# 케라스 원-핫 인코딩에서 배열의 인덱스가 0부터 시작하기 때문에\n","# 배열의 크기를 실제 단어 집합의 크기보다 +1로 생성해야하므로 미리 +1 선언\n","\n","print(vocab_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["16\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WbzKRXBz9nUt","outputId":"eaf62b97-40c0-4879-9d97-6b347c340e54"},"source":["x_encoded = t.texts_to_sequences(sentences)\n","print(x_encoded)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9aWsyke-247","outputId":"958eaaaf-ade8-4eb0-dab7-c1dbc00de16f"},"source":["max_len=max(len(l) for l in x_encoded)\n","# 가장 긴 문장을 기준으로 길이 정하기\n","print(max_len)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5DpTsjkC_q7u","outputId":"2be2e96c-c6c4-4bb9-a0d1-67e782380a0f"},"source":["x_train = pad_sequences(x_encoded, maxlen=max_len, padding='post')\n","# 가장 긴문장을 기준으로 나머지는 0으로 채우는 패딩\n","\n","y_train=np.array(y_train)\n","print(x_train)\n","print(y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 1  2  3  4]\n"," [ 5  6  0  0]\n"," [ 7  8  0  0]\n"," [ 9 10  0  0]\n"," [11 12  0  0]\n"," [13  0  0  0]\n"," [14 15  0  0]]\n","[1 0 0 1 1 0 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c08OF1yAANiv"},"source":["# 모델 설계\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, Flatten\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, 4, input_length=max_len))\n","model.add(Flatten()) # 플래튼(Flatten) 레이어는 인풋의 차원을 풀어 일렬화 한다. # Dense의 입력으로 넣기위함.\n","# https://tykimos.github.io/2017/01/27/CNN_Layer_Talk/\n","model.add(Dense(1, activation='sigmoid'))\n","# 출력층에 1개의 뉴런에 활성화 함수로는 시그모이드 함수를 사용하여 이진 분류를 수행"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2Gn_X_9C6eI"},"source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","model.fit(x_train, y_train, epochs=100, verbose=2)\n","# 각 단어들의 임베딩 벡터들의 값은 학습 과정에서 다른 가중치들과 함께 학습된 값이다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I56BdYKIE4Y_"},"source":["### 4) 사전 훈련된 워드 임베딩(Pre-Trained Word Embedding) 사용하기"]},{"cell_type":"code","metadata":{"id":"OtiLLYGnDRT_"},"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove*.zip\n","\n","'''\n","Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt\n","  inflating: glove.6B.100d.txt\n","  inflating: glove.6B.200d.txt\n","  inflating: glove.6B.300d.txt '''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NwUN8TUZFhDz"},"source":["# glove.6B.100d.txt 파일에는 하나의 줄당 101개의 값을 가지는 리스트를 갖고 있다.\n","n = 0\n","f = open('glove.6B.100d.txt', encoding=\"utf8\")\n","\n","for line in f:\n","    word_vector = line.split()\n","    print(word_vector) # 각 줄을 출력\n","    word = word_vector[0]\n","    print(word) # word_vector의 첫번째 값만 출력\n","    n = n+1\n","    if n==10:\n","        break\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqF1wcFuGQqX","outputId":"29a0d642-0acd-4de9-e0f6-faa16654ef98"},"source":["print(type(word_vector))\n","print(len(word_vector))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'list'>\n","101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAUqbYUBGfvl","outputId":"5a3df97b-f40f-46ba-ff0b-7d2a154a469d"},"source":["# glove.6B.100d.txt에 있는 모든 임베딩 벡터들을 불러와 키(key)와 값(value)의 쌍(pair) 구조로 저장하기\n","\n","embedding_dict = dict()\n","f = open('glove.6B.100d.txt', encoding=\"utf8\")\n","\n","for line in f:\n","    word_vector = line.split()\n","    word = word_vector[0]\n","    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n","    # 100개의 값을 가지는 array로 변환\n","    embedding_dict[word] = word_vector_arr #임베딩 벡터가 의미하는 단어들을 모아놓음\n","f.close()\n","print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["400000개의 Embedding vector가 있습니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hImgU3yHKiwF","outputId":"e222a69b-08b7-4175-c8ab-350795c61c37"},"source":["# 임의의 단어 excellent의 임베딩 벡터 값 출력\n","print(embedding_dict['excellent'])\n","print(len(embedding_dict['excellent']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.2816     0.18427   -0.06755    0.27694   -0.066775  -0.41389\n","  0.30757   -0.11097   -0.84585   -0.17047    0.0062422 -0.65395\n","  0.28771   -0.12409   -0.26717    0.026893   0.17115   -0.46256\n","  0.19549    1.1399    -0.46206    0.39222   -0.18622   -0.53259\n","  0.073297   0.0045262 -0.45476    0.16952   -0.41111   -0.31766\n"," -0.73616    0.56228   -0.26528   -0.088054   0.93175    0.46633\n"," -0.36245    0.46954   -0.18022   -0.07036    0.55793    0.13965\n","  0.38983   -0.04636    0.55198    0.020288   0.34741   -0.61839\n","  0.051404  -0.83699    0.05628   -0.24738   -0.19872    0.75093\n","  0.068948  -1.7575     0.70621    0.079792   1.1462    -0.63423\n"," -0.68378    0.51682   -0.68141    0.29451    0.25864   -0.25839\n","  0.36467    0.026786   0.35151   -0.13273    0.18513    0.52367\n","  1.0416    -0.12293    0.77957    0.41305   -0.28948    0.19722\n","  0.41088   -0.35153    0.48837    0.58854   -0.37273   -0.45631\n"," -0.98976   -0.070993   0.48845   -0.082542  -0.49009   -0.29484\n","  0.031072  -0.38779    0.15321   -0.0078053 -0.43276    0.37334\n"," -0.76888   -0.82518    0.28753    0.76069  ]\n","100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xgOY0l1WLAlU","outputId":"b7b98685-e416-484b-ba73-8f1c734317eb"},"source":["embedding_matrix = np.zeros((vocab_size, 100))\n","# 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.\n","# vocab_size는 앞서 정의한 15+1이다.\n","np.shape(embedding_matrix)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(16, 100)"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vdsz_qUEMAWc","outputId":"ae307b7e-9a90-42d4-daf5-3d3d89cae695"},"source":["print(t.word_index.items())\n","# t.word_index는 각 단어에 대한 정수값 부여(1~15)한 것"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zm8MTveJMR10"},"source":["for word, i in t.word_index.items():\n","    temp = embedding_dict.get(word)  # 단어(key) 해당되는 임베딩 벡터의 100개의 값(value)를 임시 변수에 저장\n","    if temp is not None:\n","        embedding_matrix[i] = temp # 임수 변수의 값을 단어와 맵핑되는 인덱스의 행에 삽입"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugrAXetENeTi","outputId":"b3222354-2063-475f-e55a-b2c30b43c52c"},"source":["temp"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.049773 ,  0.19903  ,  0.10585  ,  0.1391   , -0.32395  ,\n","        0.44053  ,  0.3947   , -0.22805  , -0.25793  ,  0.49768  ,\n","        0.15384  , -0.08831  ,  0.0782   , -0.8299   , -0.037788 ,\n","        0.16772  , -0.45197  , -0.17085  ,  0.74756  ,  0.98256  ,\n","        0.81872  ,  0.28507  ,  0.16178  , -0.48626  , -0.006265 ,\n","       -0.92469  , -0.30625  , -0.067318 , -0.046762 , -0.76291  ,\n","       -0.0025264, -0.018795 ,  0.12882  , -0.52457  ,  0.3586   ,\n","        0.43119  , -0.89477  , -0.057421 , -0.53724  ,  0.25587  ,\n","        0.55195  ,  0.44698  , -0.24252  ,  0.29946  ,  0.25776  ,\n","       -0.8717   ,  0.68426  , -0.05688  , -0.1848   , -0.59352  ,\n","       -0.11227  , -0.57692  , -0.013593 ,  0.18488  , -0.32507  ,\n","       -0.90171  ,  0.17672  ,  0.075601 ,  0.54896  , -0.21488  ,\n","       -0.54018  , -0.45882  , -0.79536  ,  0.26331  ,  0.18879  ,\n","       -0.16363  ,  0.3975   ,  0.1099   ,  0.1164   , -0.083499 ,\n","        0.50159  ,  0.35802  ,  0.25677  ,  0.088546 ,  0.42108  ,\n","        0.28674  , -0.71285  , -0.82915  ,  0.15297  , -0.82712  ,\n","        0.022112 ,  1.067    , -0.31776  ,  0.1211   , -0.069755 ,\n","       -0.61327  ,  0.27308  , -0.42638  , -0.085084 , -0.17694  ,\n","       -0.0090944,  0.1109   ,  0.62543  , -0.23682  , -0.44928  ,\n","       -0.3667   , -0.21616  , -0.19187  , -0.032502 ,  0.38025  ],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhIzWB_5NjCX","outputId":"5880879e-8a01-49ec-d457-c36d531f76af"},"source":["embedding_matrix"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n","         0.        ,  0.        ],\n","       [-0.18554001,  0.047152  ,  0.34867001, ...,  0.095473  ,\n","        -0.1142    ,  0.32743001],\n","       [-0.013786  ,  0.38216001,  0.53236002, ..., -1.04260004,\n","         0.28854999,  0.63055998],\n","       ...,\n","       [ 0.39456001, -0.24717   ,  1.03190005, ...,  0.0064973 ,\n","         0.13793001, -0.06832   ],\n","       [-0.90626001,  0.11363   , -0.050354  , ..., -0.89670998,\n","        -0.059254  , -0.058493  ],\n","       [-0.049773  ,  0.19903   ,  0.10585   , ..., -0.19187   ,\n","        -0.032502  ,  0.38025001]])"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"u3_lbMOGNs01"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, Flatten\n","\n","model = Sequential()\n","e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False)\n","\n","# 사전 훈련된 워드 임베딩을 100차원의 값인 것으로 사용하고 있기 때문에 임베딩 층의 output_dim의 인자값으로 100을 주어야 한다.\n","# 사전 훈련된 워드 임베딩을 그대로 사용할 것이므로, trainable=False로 별도로 더 이상 훈련을 하지 않는다는 옵션을 설정한다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aSDBA81pQCjQ","outputId":"5dc9e9cc-615c-47a5-c8b1-5faadd0c42f4"},"source":["model.add(e)\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","model.fit(x_train, y_train, epochs=100, verbose=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","1/1 - 0s - loss: 0.6344 - acc: 0.7143\n","Epoch 2/100\n","1/1 - 0s - loss: 0.6171 - acc: 0.7143\n","Epoch 3/100\n","1/1 - 0s - loss: 0.6003 - acc: 0.7143\n","Epoch 4/100\n","1/1 - 0s - loss: 0.5841 - acc: 0.7143\n","Epoch 5/100\n","1/1 - 0s - loss: 0.5684 - acc: 0.7143\n","Epoch 6/100\n","1/1 - 0s - loss: 0.5532 - acc: 0.7143\n","Epoch 7/100\n","1/1 - 0s - loss: 0.5386 - acc: 0.7143\n","Epoch 8/100\n","1/1 - 0s - loss: 0.5244 - acc: 0.7143\n","Epoch 9/100\n","1/1 - 0s - loss: 0.5107 - acc: 0.7143\n","Epoch 10/100\n","1/1 - 0s - loss: 0.4975 - acc: 0.7143\n","Epoch 11/100\n","1/1 - 0s - loss: 0.4847 - acc: 0.8571\n","Epoch 12/100\n","1/1 - 0s - loss: 0.4723 - acc: 0.8571\n","Epoch 13/100\n","1/1 - 0s - loss: 0.4604 - acc: 0.8571\n","Epoch 14/100\n","1/1 - 0s - loss: 0.4488 - acc: 0.8571\n","Epoch 15/100\n","1/1 - 0s - loss: 0.4376 - acc: 0.8571\n","Epoch 16/100\n","1/1 - 0s - loss: 0.4268 - acc: 1.0000\n","Epoch 17/100\n","1/1 - 0s - loss: 0.4163 - acc: 1.0000\n","Epoch 18/100\n","1/1 - 0s - loss: 0.4062 - acc: 1.0000\n","Epoch 19/100\n","1/1 - 0s - loss: 0.3963 - acc: 1.0000\n","Epoch 20/100\n","1/1 - 0s - loss: 0.3868 - acc: 1.0000\n","Epoch 21/100\n","1/1 - 0s - loss: 0.3776 - acc: 1.0000\n","Epoch 22/100\n","1/1 - 0s - loss: 0.3687 - acc: 1.0000\n","Epoch 23/100\n","1/1 - 0s - loss: 0.3600 - acc: 1.0000\n","Epoch 24/100\n","1/1 - 0s - loss: 0.3516 - acc: 1.0000\n","Epoch 25/100\n","1/1 - 0s - loss: 0.3435 - acc: 1.0000\n","Epoch 26/100\n","1/1 - 0s - loss: 0.3356 - acc: 1.0000\n","Epoch 27/100\n","1/1 - 0s - loss: 0.3279 - acc: 1.0000\n","Epoch 28/100\n","1/1 - 0s - loss: 0.3205 - acc: 1.0000\n","Epoch 29/100\n","1/1 - 0s - loss: 0.3133 - acc: 1.0000\n","Epoch 30/100\n","1/1 - 0s - loss: 0.3063 - acc: 1.0000\n","Epoch 31/100\n","1/1 - 0s - loss: 0.2995 - acc: 1.0000\n","Epoch 32/100\n","1/1 - 0s - loss: 0.2929 - acc: 1.0000\n","Epoch 33/100\n","1/1 - 0s - loss: 0.2864 - acc: 1.0000\n","Epoch 34/100\n","1/1 - 0s - loss: 0.2802 - acc: 1.0000\n","Epoch 35/100\n","1/1 - 0s - loss: 0.2741 - acc: 1.0000\n","Epoch 36/100\n","1/1 - 0s - loss: 0.2683 - acc: 1.0000\n","Epoch 37/100\n","1/1 - 0s - loss: 0.2626 - acc: 1.0000\n","Epoch 38/100\n","1/1 - 0s - loss: 0.2570 - acc: 1.0000\n","Epoch 39/100\n","1/1 - 0s - loss: 0.2516 - acc: 1.0000\n","Epoch 40/100\n","1/1 - 0s - loss: 0.2464 - acc: 1.0000\n","Epoch 41/100\n","1/1 - 0s - loss: 0.2413 - acc: 1.0000\n","Epoch 42/100\n","1/1 - 0s - loss: 0.2363 - acc: 1.0000\n","Epoch 43/100\n","1/1 - 0s - loss: 0.2315 - acc: 1.0000\n","Epoch 44/100\n","1/1 - 0s - loss: 0.2269 - acc: 1.0000\n","Epoch 45/100\n","1/1 - 0s - loss: 0.2223 - acc: 1.0000\n","Epoch 46/100\n","1/1 - 0s - loss: 0.2179 - acc: 1.0000\n","Epoch 47/100\n","1/1 - 0s - loss: 0.2137 - acc: 1.0000\n","Epoch 48/100\n","1/1 - 0s - loss: 0.2095 - acc: 1.0000\n","Epoch 49/100\n","1/1 - 0s - loss: 0.2054 - acc: 1.0000\n","Epoch 50/100\n","1/1 - 0s - loss: 0.2015 - acc: 1.0000\n","Epoch 51/100\n","1/1 - 0s - loss: 0.1977 - acc: 1.0000\n","Epoch 52/100\n","1/1 - 0s - loss: 0.1940 - acc: 1.0000\n","Epoch 53/100\n","1/1 - 0s - loss: 0.1904 - acc: 1.0000\n","Epoch 54/100\n","1/1 - 0s - loss: 0.1868 - acc: 1.0000\n","Epoch 55/100\n","1/1 - 0s - loss: 0.1834 - acc: 1.0000\n","Epoch 56/100\n","1/1 - 0s - loss: 0.1801 - acc: 1.0000\n","Epoch 57/100\n","1/1 - 0s - loss: 0.1769 - acc: 1.0000\n","Epoch 58/100\n","1/1 - 0s - loss: 0.1737 - acc: 1.0000\n","Epoch 59/100\n","1/1 - 0s - loss: 0.1707 - acc: 1.0000\n","Epoch 60/100\n","1/1 - 0s - loss: 0.1677 - acc: 1.0000\n","Epoch 61/100\n","1/1 - 0s - loss: 0.1648 - acc: 1.0000\n","Epoch 62/100\n","1/1 - 0s - loss: 0.1619 - acc: 1.0000\n","Epoch 63/100\n","1/1 - 0s - loss: 0.1592 - acc: 1.0000\n","Epoch 64/100\n","1/1 - 0s - loss: 0.1565 - acc: 1.0000\n","Epoch 65/100\n","1/1 - 0s - loss: 0.1539 - acc: 1.0000\n","Epoch 66/100\n","1/1 - 0s - loss: 0.1513 - acc: 1.0000\n","Epoch 67/100\n","1/1 - 0s - loss: 0.1489 - acc: 1.0000\n","Epoch 68/100\n","1/1 - 0s - loss: 0.1464 - acc: 1.0000\n","Epoch 69/100\n","1/1 - 0s - loss: 0.1441 - acc: 1.0000\n","Epoch 70/100\n","1/1 - 0s - loss: 0.1418 - acc: 1.0000\n","Epoch 71/100\n","1/1 - 0s - loss: 0.1395 - acc: 1.0000\n","Epoch 72/100\n","1/1 - 0s - loss: 0.1373 - acc: 1.0000\n","Epoch 73/100\n","1/1 - 0s - loss: 0.1352 - acc: 1.0000\n","Epoch 74/100\n","1/1 - 0s - loss: 0.1331 - acc: 1.0000\n","Epoch 75/100\n","1/1 - 0s - loss: 0.1311 - acc: 1.0000\n","Epoch 76/100\n","1/1 - 0s - loss: 0.1291 - acc: 1.0000\n","Epoch 77/100\n","1/1 - 0s - loss: 0.1272 - acc: 1.0000\n","Epoch 78/100\n","1/1 - 0s - loss: 0.1253 - acc: 1.0000\n","Epoch 79/100\n","1/1 - 0s - loss: 0.1234 - acc: 1.0000\n","Epoch 80/100\n","1/1 - 0s - loss: 0.1216 - acc: 1.0000\n","Epoch 81/100\n","1/1 - 0s - loss: 0.1199 - acc: 1.0000\n","Epoch 82/100\n","1/1 - 0s - loss: 0.1181 - acc: 1.0000\n","Epoch 83/100\n","1/1 - 0s - loss: 0.1165 - acc: 1.0000\n","Epoch 84/100\n","1/1 - 0s - loss: 0.1148 - acc: 1.0000\n","Epoch 85/100\n","1/1 - 0s - loss: 0.1132 - acc: 1.0000\n","Epoch 86/100\n","1/1 - 0s - loss: 0.1116 - acc: 1.0000\n","Epoch 87/100\n","1/1 - 0s - loss: 0.1101 - acc: 1.0000\n","Epoch 88/100\n","1/1 - 0s - loss: 0.1086 - acc: 1.0000\n","Epoch 89/100\n","1/1 - 0s - loss: 0.1071 - acc: 1.0000\n","Epoch 90/100\n","1/1 - 0s - loss: 0.1057 - acc: 1.0000\n","Epoch 91/100\n","1/1 - 0s - loss: 0.1043 - acc: 1.0000\n","Epoch 92/100\n","1/1 - 0s - loss: 0.1029 - acc: 1.0000\n","Epoch 93/100\n","1/1 - 0s - loss: 0.1015 - acc: 1.0000\n","Epoch 94/100\n","1/1 - 0s - loss: 0.1002 - acc: 1.0000\n","Epoch 95/100\n","1/1 - 0s - loss: 0.0989 - acc: 1.0000\n","Epoch 96/100\n","1/1 - 0s - loss: 0.0977 - acc: 1.0000\n","Epoch 97/100\n","1/1 - 0s - loss: 0.0964 - acc: 1.0000\n","Epoch 98/100\n","1/1 - 0s - loss: 0.0952 - acc: 1.0000\n","Epoch 99/100\n","1/1 - 0s - loss: 0.0940 - acc: 1.0000\n","Epoch 100/100\n","1/1 - 0s - loss: 0.0928 - acc: 1.0000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f0c96afe390>"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"ITEXmVrxRkGD"},"source":["- 사전 훈련된 Word2Vec 사용하기\n"]},{"cell_type":"code","metadata":{"id":"seodIZsCRSag"},"source":["import numpy as np\n","import gensim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SDZpUoPKRpR8","outputId":"fcd67600-ac03-4da7-e4ba-b0c6d050e82a"},"source":["# 현재 위치에 구글의 사전 훈련된 Word2Vec을 다운로드\n","!wget \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-04-08 06:58:44--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.86.37\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.86.37|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1647046227 (1.5G) [application/x-gzip]\n","Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n","\n","GoogleNews-vectors- 100%[===================>]   1.53G  71.0MB/s    in 24s     \n","\n","2021-04-08 06:59:08 (65.5 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YAld6jmWRqkq"},"source":["# 구글의 사전 훈련된 Word2vec 모델을 로드\n","word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OuBHsuIlRvox","outputId":"8ed9c4c6-5858-42b6-b29e-ebd4b5f746a5"},"source":["print(word2vec_model.vectors.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(3000000, 300)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tUmZyV72R23M","outputId":"9f31002f-e84e-4ada-dc49-fa668e13619c"},"source":["embedding_matrix = np.zeros((vocab_size, 300))\n","\n","np.shape(embedding_matrix)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(16, 300)"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"aAIbSHTySC92"},"source":["def get_vector(word):\n","    if word in word2vec_model:\n","        return word2vec_model[word]\n","    else:\n","        return None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11A-5dACSbGB","outputId":"280d9eb2-57c4-4cc3-993b-2acd1c725aa2"},"source":["for word, i in t.word_index.items():\n","    # 훈련 데이터의 단어 집합에서 단어와 정수 인덱스를 1개씩 꺼내온다.\n","    temp = get_vector(word)\n","    # 단어(key) 해당되는 임베딩 벡터의 300개의 값(value)를 임시 변수에 저장\n","    if temp is not None:\n","        embedding_matrix[i] = temp\n","        # 해당 단어 위치의 행에 벡터의 값을 저장한다.\n","\n","print(word2vec_model['excellent'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-2.12890625e-01 -4.30297852e-03 -1.80664062e-01 -7.56835938e-03\n","  1.12792969e-01  1.63085938e-01 -1.47094727e-02 -7.86132812e-02\n"," -1.64062500e-01  2.79296875e-01 -2.22656250e-01 -1.37329102e-02\n","  1.45507812e-01 -4.56542969e-02  5.85937500e-02  1.85546875e-01\n","  1.13769531e-01  1.13769531e-01 -1.46484375e-01 -1.25122070e-02\n","  8.78906250e-02  1.44531250e-01 -6.88476562e-02  1.68945312e-01\n","  2.02148438e-01  3.64685059e-03 -1.24023438e-01  1.76757812e-01\n"," -4.24804688e-02 -3.54003906e-03  4.93164062e-02 -1.74804688e-01\n","  2.53906250e-01  1.32812500e-01  2.63671875e-01  2.87109375e-01\n"," -1.98242188e-01 -9.81445312e-02 -1.18652344e-01 -1.43554688e-01\n","  3.08593750e-01 -8.93554688e-02 -7.17163086e-03  3.46679688e-02\n"," -7.86132812e-02 -1.50390625e-01  2.15820312e-01 -1.17675781e-01\n","  2.85644531e-02 -2.41210938e-01  1.22558594e-01  1.28906250e-01\n"," -9.27734375e-02  1.16210938e-01  3.29589844e-02  6.49414062e-02\n"," -8.20312500e-02 -1.60156250e-01  2.00195312e-01 -2.68554688e-02\n"," -1.57226562e-01  1.26953125e-01 -5.42968750e-01 -3.24218750e-01\n"," -4.27246094e-03  1.14746094e-01 -1.59179688e-01 -2.71484375e-01\n"," -2.23632812e-01  1.78710938e-01 -1.20117188e-01  4.78515625e-02\n","  1.81640625e-01  2.68554688e-02 -1.33789062e-01  3.14941406e-02\n"," -1.73339844e-02  2.90527344e-02  1.13281250e-01  3.95507812e-02\n"," -1.63085938e-01  1.17187500e-01 -4.88281250e-02  1.16699219e-01\n","  6.25000000e-02 -2.07519531e-02 -1.59179688e-01  9.27734375e-02\n"," -9.08203125e-02  1.59912109e-02  6.73828125e-02 -1.61132812e-01\n"," -7.51953125e-02  2.10937500e-01 -6.00585938e-02 -6.68945312e-02\n"," -1.07910156e-01  2.91015625e-01 -7.81250000e-02 -2.55859375e-01\n","  7.93457031e-03  3.02734375e-02 -1.39648438e-01 -8.30078125e-02\n","  4.32128906e-02  1.09375000e-01 -1.85546875e-02 -2.52685547e-02\n"," -1.19140625e-01 -1.89453125e-01 -5.66406250e-02  1.58203125e-01\n"," -2.04101562e-01 -7.95898438e-02  2.47070312e-01 -1.71875000e-01\n","  9.37500000e-02 -2.26562500e-01 -2.75878906e-02 -1.04003906e-01\n"," -4.10156250e-02 -1.82617188e-01  2.67578125e-01  1.37695312e-01\n"," -1.04980469e-01  5.79833984e-03 -1.53320312e-01 -2.26562500e-01\n"," -7.41577148e-03  4.06250000e-01 -1.91406250e-01  1.12304688e-02\n"," -2.10937500e-01  2.10937500e-01 -8.16345215e-04 -6.20117188e-02\n","  1.22070312e-01 -9.65118408e-04 -2.60009766e-02  4.31640625e-01\n"," -1.76757812e-01  2.53906250e-01  5.41992188e-02  2.27539062e-01\n"," -2.08984375e-01 -1.22558594e-01 -6.05468750e-02 -1.24511719e-01\n","  2.29492188e-01  3.93066406e-02  1.64062500e-01  1.52343750e-01\n"," -8.49609375e-02  2.73437500e-01 -9.66796875e-02 -8.44726562e-02\n","  1.63574219e-02 -5.51757812e-02 -3.61328125e-02 -8.78906250e-02\n"," -1.11816406e-01  1.18652344e-01  8.49609375e-02  1.42578125e-01\n","  3.12500000e-01 -4.49218750e-02 -4.17480469e-02 -2.00195312e-01\n"," -8.64257812e-02 -1.95312500e-01 -9.47265625e-02 -4.91333008e-03\n","  1.16699219e-01  7.86132812e-02 -2.76184082e-03  1.19140625e-01\n"," -5.27954102e-03 -1.91406250e-01 -4.71191406e-02 -2.38037109e-02\n"," -2.49023438e-01 -3.18359375e-01  1.29882812e-01 -1.21093750e-01\n"," -1.80664062e-01 -1.00585938e-01 -2.47070312e-01 -1.13769531e-01\n"," -1.13281250e-01 -5.81054688e-02 -4.02343750e-01  6.29882812e-02\n","  1.28906250e-01 -1.54296875e-01  5.78613281e-02 -1.68945312e-01\n","  1.72851562e-01 -2.29492188e-01 -8.44726562e-02 -8.69140625e-02\n","  4.34570312e-02  2.29492188e-01 -5.00488281e-02 -1.13769531e-01\n","  1.35742188e-01 -8.88671875e-02 -6.64062500e-02 -1.26647949e-03\n"," -1.86767578e-02  1.66992188e-01  5.78613281e-02  8.36181641e-03\n"," -2.83203125e-01  9.86328125e-02  6.83593750e-02 -1.34765625e-01\n","  3.69140625e-01  6.64062500e-02  2.24609375e-01 -1.43554688e-01\n"," -7.42187500e-02  9.47265625e-02 -2.43164062e-01 -1.26342773e-02\n","  1.82617188e-01  1.04980469e-01 -9.09423828e-03 -1.95312500e-01\n","  5.02929688e-02 -1.33789062e-01 -8.34960938e-02 -1.86523438e-01\n","  1.56250000e-01 -3.44238281e-02 -5.15136719e-02 -3.90625000e-02\n"," -9.17968750e-02 -3.95507812e-02 -9.61914062e-02 -1.97753906e-02\n"," -9.86328125e-02  5.32226562e-02  1.09375000e-01  6.73828125e-02\n","  7.17773438e-02  6.59179688e-02  1.44653320e-02 -1.27929688e-01\n"," -2.35351562e-01 -3.18359375e-01  5.79833984e-03  5.88378906e-02\n"," -2.86865234e-02  2.53906250e-01  1.71875000e-01 -2.24609375e-01\n"," -1.17187500e-02  3.73535156e-02 -1.58203125e-01 -3.90625000e-02\n","  3.78417969e-02  1.10839844e-01 -8.44726562e-02  5.56640625e-02\n","  1.60156250e-01 -5.41992188e-02 -5.98144531e-03 -2.81250000e-01\n"," -1.14135742e-02 -1.15234375e-01 -1.09863281e-01 -3.58886719e-02\n"," -1.98242188e-01 -8.44726562e-02  5.27343750e-02 -2.44140625e-03\n","  2.65625000e-01 -2.34375000e-01 -8.74023438e-02  1.81640625e-01\n","  3.97949219e-02  2.72216797e-02 -1.22558594e-01  1.79687500e-01\n"," -2.69531250e-01  1.31835938e-01 -1.28906250e-01 -6.88476562e-02\n"," -3.18908691e-03 -1.43432617e-02  1.36718750e-01  2.82287598e-04\n"," -1.73828125e-01  4.24194336e-03 -8.10546875e-02  1.35498047e-02\n"," -8.36181641e-03 -1.29882812e-01 -2.15820312e-01  1.22680664e-02]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5belOArdUFar","outputId":"19720a92-d221-4444-b48f-842ea565b2a6"},"source":["print('단어 excellent의 정수 인덱스: ', t.word_index['excellent'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["단어 excellent의 정수 인덱스:  9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3CrLNuJUSUO","outputId":"9fa1e4fb-f6a5-4240-cd51-fa26d2cfe64a"},"source":["# 9의 값을 가지므로 embedding_matirx의 1번 인덱스에는 단어 'excellent'의 임베딩 벡터값이 있어야 한다.\n","print(embedding_matrix[9])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-2.12890625e-01 -4.30297852e-03 -1.80664062e-01 -7.56835938e-03\n","  1.12792969e-01  1.63085938e-01 -1.47094727e-02 -7.86132812e-02\n"," -1.64062500e-01  2.79296875e-01 -2.22656250e-01 -1.37329102e-02\n","  1.45507812e-01 -4.56542969e-02  5.85937500e-02  1.85546875e-01\n","  1.13769531e-01  1.13769531e-01 -1.46484375e-01 -1.25122070e-02\n","  8.78906250e-02  1.44531250e-01 -6.88476562e-02  1.68945312e-01\n","  2.02148438e-01  3.64685059e-03 -1.24023438e-01  1.76757812e-01\n"," -4.24804688e-02 -3.54003906e-03  4.93164062e-02 -1.74804688e-01\n","  2.53906250e-01  1.32812500e-01  2.63671875e-01  2.87109375e-01\n"," -1.98242188e-01 -9.81445312e-02 -1.18652344e-01 -1.43554688e-01\n","  3.08593750e-01 -8.93554688e-02 -7.17163086e-03  3.46679688e-02\n"," -7.86132812e-02 -1.50390625e-01  2.15820312e-01 -1.17675781e-01\n","  2.85644531e-02 -2.41210938e-01  1.22558594e-01  1.28906250e-01\n"," -9.27734375e-02  1.16210938e-01  3.29589844e-02  6.49414062e-02\n"," -8.20312500e-02 -1.60156250e-01  2.00195312e-01 -2.68554688e-02\n"," -1.57226562e-01  1.26953125e-01 -5.42968750e-01 -3.24218750e-01\n"," -4.27246094e-03  1.14746094e-01 -1.59179688e-01 -2.71484375e-01\n"," -2.23632812e-01  1.78710938e-01 -1.20117188e-01  4.78515625e-02\n","  1.81640625e-01  2.68554688e-02 -1.33789062e-01  3.14941406e-02\n"," -1.73339844e-02  2.90527344e-02  1.13281250e-01  3.95507812e-02\n"," -1.63085938e-01  1.17187500e-01 -4.88281250e-02  1.16699219e-01\n","  6.25000000e-02 -2.07519531e-02 -1.59179688e-01  9.27734375e-02\n"," -9.08203125e-02  1.59912109e-02  6.73828125e-02 -1.61132812e-01\n"," -7.51953125e-02  2.10937500e-01 -6.00585938e-02 -6.68945312e-02\n"," -1.07910156e-01  2.91015625e-01 -7.81250000e-02 -2.55859375e-01\n","  7.93457031e-03  3.02734375e-02 -1.39648438e-01 -8.30078125e-02\n","  4.32128906e-02  1.09375000e-01 -1.85546875e-02 -2.52685547e-02\n"," -1.19140625e-01 -1.89453125e-01 -5.66406250e-02  1.58203125e-01\n"," -2.04101562e-01 -7.95898438e-02  2.47070312e-01 -1.71875000e-01\n","  9.37500000e-02 -2.26562500e-01 -2.75878906e-02 -1.04003906e-01\n"," -4.10156250e-02 -1.82617188e-01  2.67578125e-01  1.37695312e-01\n"," -1.04980469e-01  5.79833984e-03 -1.53320312e-01 -2.26562500e-01\n"," -7.41577148e-03  4.06250000e-01 -1.91406250e-01  1.12304688e-02\n"," -2.10937500e-01  2.10937500e-01 -8.16345215e-04 -6.20117188e-02\n","  1.22070312e-01 -9.65118408e-04 -2.60009766e-02  4.31640625e-01\n"," -1.76757812e-01  2.53906250e-01  5.41992188e-02  2.27539062e-01\n"," -2.08984375e-01 -1.22558594e-01 -6.05468750e-02 -1.24511719e-01\n","  2.29492188e-01  3.93066406e-02  1.64062500e-01  1.52343750e-01\n"," -8.49609375e-02  2.73437500e-01 -9.66796875e-02 -8.44726562e-02\n","  1.63574219e-02 -5.51757812e-02 -3.61328125e-02 -8.78906250e-02\n"," -1.11816406e-01  1.18652344e-01  8.49609375e-02  1.42578125e-01\n","  3.12500000e-01 -4.49218750e-02 -4.17480469e-02 -2.00195312e-01\n"," -8.64257812e-02 -1.95312500e-01 -9.47265625e-02 -4.91333008e-03\n","  1.16699219e-01  7.86132812e-02 -2.76184082e-03  1.19140625e-01\n"," -5.27954102e-03 -1.91406250e-01 -4.71191406e-02 -2.38037109e-02\n"," -2.49023438e-01 -3.18359375e-01  1.29882812e-01 -1.21093750e-01\n"," -1.80664062e-01 -1.00585938e-01 -2.47070312e-01 -1.13769531e-01\n"," -1.13281250e-01 -5.81054688e-02 -4.02343750e-01  6.29882812e-02\n","  1.28906250e-01 -1.54296875e-01  5.78613281e-02 -1.68945312e-01\n","  1.72851562e-01 -2.29492188e-01 -8.44726562e-02 -8.69140625e-02\n","  4.34570312e-02  2.29492188e-01 -5.00488281e-02 -1.13769531e-01\n","  1.35742188e-01 -8.88671875e-02 -6.64062500e-02 -1.26647949e-03\n"," -1.86767578e-02  1.66992188e-01  5.78613281e-02  8.36181641e-03\n"," -2.83203125e-01  9.86328125e-02  6.83593750e-02 -1.34765625e-01\n","  3.69140625e-01  6.64062500e-02  2.24609375e-01 -1.43554688e-01\n"," -7.42187500e-02  9.47265625e-02 -2.43164062e-01 -1.26342773e-02\n","  1.82617188e-01  1.04980469e-01 -9.09423828e-03 -1.95312500e-01\n","  5.02929688e-02 -1.33789062e-01 -8.34960938e-02 -1.86523438e-01\n","  1.56250000e-01 -3.44238281e-02 -5.15136719e-02 -3.90625000e-02\n"," -9.17968750e-02 -3.95507812e-02 -9.61914062e-02 -1.97753906e-02\n"," -9.86328125e-02  5.32226562e-02  1.09375000e-01  6.73828125e-02\n","  7.17773438e-02  6.59179688e-02  1.44653320e-02 -1.27929688e-01\n"," -2.35351562e-01 -3.18359375e-01  5.79833984e-03  5.88378906e-02\n"," -2.86865234e-02  2.53906250e-01  1.71875000e-01 -2.24609375e-01\n"," -1.17187500e-02  3.73535156e-02 -1.58203125e-01 -3.90625000e-02\n","  3.78417969e-02  1.10839844e-01 -8.44726562e-02  5.56640625e-02\n","  1.60156250e-01 -5.41992188e-02 -5.98144531e-03 -2.81250000e-01\n"," -1.14135742e-02 -1.15234375e-01 -1.09863281e-01 -3.58886719e-02\n"," -1.98242188e-01 -8.44726562e-02  5.27343750e-02 -2.44140625e-03\n","  2.65625000e-01 -2.34375000e-01 -8.74023438e-02  1.81640625e-01\n","  3.97949219e-02  2.72216797e-02 -1.22558594e-01  1.79687500e-01\n"," -2.69531250e-01  1.31835938e-01 -1.28906250e-01 -6.88476562e-02\n"," -3.18908691e-03 -1.43432617e-02  1.36718750e-01  2.82287598e-04\n"," -1.73828125e-01  4.24194336e-03 -8.10546875e-02  1.35498047e-02\n"," -8.36181641e-03 -1.29882812e-01 -2.15820312e-01  1.22680664e-02]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"laTSt_93Ue7J","outputId":"3a515948-a1ad-4329-c2fe-1b9b8745620a"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, Flatten\n","\n","model = Sequential()\n","e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\n","model.add(e)\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","model.fit(x_train, y_train, epochs=100, verbose=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","1/1 - 0s - loss: 0.7170 - acc: 0.4286\n","Epoch 2/100\n","1/1 - 0s - loss: 0.6976 - acc: 0.5714\n","Epoch 3/100\n","1/1 - 0s - loss: 0.6788 - acc: 0.7143\n","Epoch 4/100\n","1/1 - 0s - loss: 0.6605 - acc: 0.8571\n","Epoch 5/100\n","1/1 - 0s - loss: 0.6427 - acc: 0.8571\n","Epoch 6/100\n","1/1 - 0s - loss: 0.6256 - acc: 0.8571\n","Epoch 7/100\n","1/1 - 0s - loss: 0.6090 - acc: 0.8571\n","Epoch 8/100\n","1/1 - 0s - loss: 0.5929 - acc: 0.8571\n","Epoch 9/100\n","1/1 - 0s - loss: 0.5774 - acc: 0.8571\n","Epoch 10/100\n","1/1 - 0s - loss: 0.5624 - acc: 1.0000\n","Epoch 11/100\n","1/1 - 0s - loss: 0.5479 - acc: 1.0000\n","Epoch 12/100\n","1/1 - 0s - loss: 0.5340 - acc: 1.0000\n","Epoch 13/100\n","1/1 - 0s - loss: 0.5205 - acc: 1.0000\n","Epoch 14/100\n","1/1 - 0s - loss: 0.5075 - acc: 1.0000\n","Epoch 15/100\n","1/1 - 0s - loss: 0.4949 - acc: 1.0000\n","Epoch 16/100\n","1/1 - 0s - loss: 0.4828 - acc: 1.0000\n","Epoch 17/100\n","1/1 - 0s - loss: 0.4711 - acc: 1.0000\n","Epoch 18/100\n","1/1 - 0s - loss: 0.4598 - acc: 1.0000\n","Epoch 19/100\n","1/1 - 0s - loss: 0.4489 - acc: 1.0000\n","Epoch 20/100\n","1/1 - 0s - loss: 0.4383 - acc: 1.0000\n","Epoch 21/100\n","1/1 - 0s - loss: 0.4281 - acc: 1.0000\n","Epoch 22/100\n","1/1 - 0s - loss: 0.4182 - acc: 1.0000\n","Epoch 23/100\n","1/1 - 0s - loss: 0.4087 - acc: 1.0000\n","Epoch 24/100\n","1/1 - 0s - loss: 0.3994 - acc: 1.0000\n","Epoch 25/100\n","1/1 - 0s - loss: 0.3904 - acc: 1.0000\n","Epoch 26/100\n","1/1 - 0s - loss: 0.3818 - acc: 1.0000\n","Epoch 27/100\n","1/1 - 0s - loss: 0.3734 - acc: 1.0000\n","Epoch 28/100\n","1/1 - 0s - loss: 0.3652 - acc: 1.0000\n","Epoch 29/100\n","1/1 - 0s - loss: 0.3573 - acc: 1.0000\n","Epoch 30/100\n","1/1 - 0s - loss: 0.3496 - acc: 1.0000\n","Epoch 31/100\n","1/1 - 0s - loss: 0.3422 - acc: 1.0000\n","Epoch 32/100\n","1/1 - 0s - loss: 0.3350 - acc: 1.0000\n","Epoch 33/100\n","1/1 - 0s - loss: 0.3280 - acc: 1.0000\n","Epoch 34/100\n","1/1 - 0s - loss: 0.3212 - acc: 1.0000\n","Epoch 35/100\n","1/1 - 0s - loss: 0.3146 - acc: 1.0000\n","Epoch 36/100\n","1/1 - 0s - loss: 0.3082 - acc: 1.0000\n","Epoch 37/100\n","1/1 - 0s - loss: 0.3020 - acc: 1.0000\n","Epoch 38/100\n","1/1 - 0s - loss: 0.2960 - acc: 1.0000\n","Epoch 39/100\n","1/1 - 0s - loss: 0.2901 - acc: 1.0000\n","Epoch 40/100\n","1/1 - 0s - loss: 0.2844 - acc: 1.0000\n","Epoch 41/100\n","1/1 - 0s - loss: 0.2789 - acc: 1.0000\n","Epoch 42/100\n","1/1 - 0s - loss: 0.2735 - acc: 1.0000\n","Epoch 43/100\n","1/1 - 0s - loss: 0.2683 - acc: 1.0000\n","Epoch 44/100\n","1/1 - 0s - loss: 0.2633 - acc: 1.0000\n","Epoch 45/100\n","1/1 - 0s - loss: 0.2583 - acc: 1.0000\n","Epoch 46/100\n","1/1 - 0s - loss: 0.2535 - acc: 1.0000\n","Epoch 47/100\n","1/1 - 0s - loss: 0.2489 - acc: 1.0000\n","Epoch 48/100\n","1/1 - 0s - loss: 0.2443 - acc: 1.0000\n","Epoch 49/100\n","1/1 - 0s - loss: 0.2399 - acc: 1.0000\n","Epoch 50/100\n","1/1 - 0s - loss: 0.2356 - acc: 1.0000\n","Epoch 51/100\n","1/1 - 0s - loss: 0.2314 - acc: 1.0000\n","Epoch 52/100\n","1/1 - 0s - loss: 0.2273 - acc: 1.0000\n","Epoch 53/100\n","1/1 - 0s - loss: 0.2234 - acc: 1.0000\n","Epoch 54/100\n","1/1 - 0s - loss: 0.2195 - acc: 1.0000\n","Epoch 55/100\n","1/1 - 0s - loss: 0.2158 - acc: 1.0000\n","Epoch 56/100\n","1/1 - 0s - loss: 0.2121 - acc: 1.0000\n","Epoch 57/100\n","1/1 - 0s - loss: 0.2085 - acc: 1.0000\n","Epoch 58/100\n","1/1 - 0s - loss: 0.2051 - acc: 1.0000\n","Epoch 59/100\n","1/1 - 0s - loss: 0.2017 - acc: 1.0000\n","Epoch 60/100\n","1/1 - 0s - loss: 0.1984 - acc: 1.0000\n","Epoch 61/100\n","1/1 - 0s - loss: 0.1951 - acc: 1.0000\n","Epoch 62/100\n","1/1 - 0s - loss: 0.1920 - acc: 1.0000\n","Epoch 63/100\n","1/1 - 0s - loss: 0.1889 - acc: 1.0000\n","Epoch 64/100\n","1/1 - 0s - loss: 0.1860 - acc: 1.0000\n","Epoch 65/100\n","1/1 - 0s - loss: 0.1830 - acc: 1.0000\n","Epoch 66/100\n","1/1 - 0s - loss: 0.1802 - acc: 1.0000\n","Epoch 67/100\n","1/1 - 0s - loss: 0.1774 - acc: 1.0000\n","Epoch 68/100\n","1/1 - 0s - loss: 0.1747 - acc: 1.0000\n","Epoch 69/100\n","1/1 - 0s - loss: 0.1721 - acc: 1.0000\n","Epoch 70/100\n","1/1 - 0s - loss: 0.1695 - acc: 1.0000\n","Epoch 71/100\n","1/1 - 0s - loss: 0.1670 - acc: 1.0000\n","Epoch 72/100\n","1/1 - 0s - loss: 0.1645 - acc: 1.0000\n","Epoch 73/100\n","1/1 - 0s - loss: 0.1621 - acc: 1.0000\n","Epoch 74/100\n","1/1 - 0s - loss: 0.1597 - acc: 1.0000\n","Epoch 75/100\n","1/1 - 0s - loss: 0.1574 - acc: 1.0000\n","Epoch 76/100\n","1/1 - 0s - loss: 0.1552 - acc: 1.0000\n","Epoch 77/100\n","1/1 - 0s - loss: 0.1530 - acc: 1.0000\n","Epoch 78/100\n","1/1 - 0s - loss: 0.1508 - acc: 1.0000\n","Epoch 79/100\n","1/1 - 0s - loss: 0.1487 - acc: 1.0000\n","Epoch 80/100\n","1/1 - 0s - loss: 0.1467 - acc: 1.0000\n","Epoch 81/100\n","1/1 - 0s - loss: 0.1446 - acc: 1.0000\n","Epoch 82/100\n","1/1 - 0s - loss: 0.1427 - acc: 1.0000\n","Epoch 83/100\n","1/1 - 0s - loss: 0.1407 - acc: 1.0000\n","Epoch 84/100\n","1/1 - 0s - loss: 0.1388 - acc: 1.0000\n","Epoch 85/100\n","1/1 - 0s - loss: 0.1370 - acc: 1.0000\n","Epoch 86/100\n","1/1 - 0s - loss: 0.1352 - acc: 1.0000\n","Epoch 87/100\n","1/1 - 0s - loss: 0.1334 - acc: 1.0000\n","Epoch 88/100\n","1/1 - 0s - loss: 0.1317 - acc: 1.0000\n","Epoch 89/100\n","1/1 - 0s - loss: 0.1300 - acc: 1.0000\n","Epoch 90/100\n","1/1 - 0s - loss: 0.1283 - acc: 1.0000\n","Epoch 91/100\n","1/1 - 0s - loss: 0.1267 - acc: 1.0000\n","Epoch 92/100\n","1/1 - 0s - loss: 0.1251 - acc: 1.0000\n","Epoch 93/100\n","1/1 - 0s - loss: 0.1235 - acc: 1.0000\n","Epoch 94/100\n","1/1 - 0s - loss: 0.1220 - acc: 1.0000\n","Epoch 95/100\n","1/1 - 0s - loss: 0.1205 - acc: 1.0000\n","Epoch 96/100\n","1/1 - 0s - loss: 0.1190 - acc: 1.0000\n","Epoch 97/100\n","1/1 - 0s - loss: 0.1175 - acc: 1.0000\n","Epoch 98/100\n","1/1 - 0s - loss: 0.1161 - acc: 1.0000\n","Epoch 99/100\n","1/1 - 0s - loss: 0.1147 - acc: 1.0000\n","Epoch 100/100\n","1/1 - 0s - loss: 0.1134 - acc: 1.0000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f0c56c8b310>"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"BfJrOv7gWOUP"},"source":["from gensim import models\n","\n","ko_model = models.fasttext.load_facebook_model('cc.ko.300.bin')\n","\n","for w, sim in m_fasttext.similar_by_word('파이썬', 10):\n","    print(f'{w}: {sim}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZqNTbMnfp-0"},"source":[],"execution_count":null,"outputs":[]}]}