{"cells":[{"cell_type":"markdown","metadata":{"id":"7Kh2PObxe3LZ"},"source":["# 2. 텍스트 전처리(Text preprocessing)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQQNUhYzpQqR","outputId":"065fb72c-88dd-45aa-bce2-d3e536af220b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TjdvmN1lqlcA","outputId":"5496dad5-4d85-4377-bb51-051cd3e95651"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"]}],"source":["from nltk.tokenize import WordPunctTokenizer\n","\n","print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xN_Don5Wqlae","outputId":"a675a2f5-0653-4e55-cf82-5e0e7a1dc34e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"]}],"source":["from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lg7j__lypQs7","outputId":"24834a6f-5552-4b14-a61e-d66ad7c77482"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["['i',\n"," 'me',\n"," 'my',\n"," 'myself',\n"," 'we',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'you',\n"," \"you're\",\n"," \"you've\",\n"," \"you'll\",\n"," \"you'd\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves',\n"," 'he',\n"," 'him',\n"," 'his']"]},"execution_count":11,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","stopwords.words('english')[:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UaGHXjj6pQzF","outputId":"9f1fc6ad-dbad-472b-a9d8-8accedd039c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["그냥 토큰화 :  ['Facebook', 'has', 'blocked', 'Australian', 'users', 'from', 'sharing', 'or', 'viewing', 'news', 'content', 'on', 'the', 'platform', ',', 'causing', 'much', 'alarm', 'over', 'public', 'access', 'to', 'key', 'information', '.']\n","토큰화 후 불용어 제거 결과 :  ['Facebook', 'blocked', 'Australian', 'users', 'sharing', 'viewing', 'news', 'content', 'platform', ',', 'causing', 'much', 'alarm', 'public', 'access', 'key', 'information', '.']\n"]}],"source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","ex = \"Facebook has blocked Australian users from sharing or viewing news content on the platform, causing much alarm over public access to key information.\"\n","stop_word = set(stopwords.words('english'))\n","\n","word_token = word_tokenize(ex)\n","\n","result = list()\n","for w in word_token:\n","    if w not in stop_word:\n","        result.append(w)\n","\n","print('그냥 토큰화 : ', word_token)\n","print('토큰화 후 불용어 제거 결과 : ', result)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ETEhumjTpQ1-","outputId":"5e65343b-993f-4d9a-df22-3e8bf1fe0532"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","['Facebook', 'ha', 'blocked', 'Australian', 'user', 'from', 'sharing', 'or', 'viewing', 'news', 'content', 'on', 'the', 'platform', 'causing', 'much', 'alarm', 'over', 'public', 'access', 'to', 'key', 'information']\n"]}],"source":["import nltk\n","nltk.download('wordnet')\n","\n","from nltk.stem import WordNetLemmatizer\n","n = WordNetLemmatizer()\n","words = ['Facebook', 'has', 'blocked', 'Australian', 'users', 'from', 'sharing', 'or', 'viewing', 'news', 'content', 'on', 'the', 'platform', 'causing', 'much', 'alarm', 'over', 'public', 'access', 'to', 'key', 'information']\n","print([n.lemmatize(w) for w in words])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"I7HWD7_DpQ5E","outputId":"e8e0e9bb-a0ec-464b-9f89-05cc3c69b3f7"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'have'"]},"execution_count":14,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["n.lemmatize('has', 'v')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xqxGt1DYpQ7J","outputId":"83ff2993-cce2-46df-a638-c4ecbf8d7aad"},"outputs":[{"name":"stdout","output_type":"stream","text":["토큰화 결과 :  ['Facebook', 'has', 'blocked', 'Australian', 'users', 'from', 'sharing', 'or', 'viewing', 'news', 'content', 'on', 'the', 'platform', ',', 'causing', 'much', 'alarm', 'over', 'public', 'access', 'to', 'key', 'information', '.']\n","어간 추출 결과 :  ['facebook', 'ha', 'block', 'australian', 'user', 'from', 'share', 'or', 'view', 'news', 'content', 'on', 'the', 'platform', ',', 'caus', 'much', 'alarm', 'over', 'public', 'access', 'to', 'key', 'inform', '.']\n"]}],"source":["from nltk.stem import PorterStemmer\n","\n","s = PorterStemmer()\n","text = \"Facebook has blocked Australian users from sharing or viewing news content on the platform, causing much alarm over public access to key information.\"\n","\n","words = word_tokenize(text)\n","print('토큰화 결과 : ', words)\n","print('어간 추출 결과 : ', [s.stem(w) for w in words])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8BAUGLqpQ9x"},"outputs":[],"source":["import re\n","\n","r =re.compile('a.c') # . 자리에 어떠한 문자가 들어오든 찾는다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dQI-c5dsdLdw","outputId":"9aa8e477-a579-42a2-acff-c23458bc246a"},"outputs":[{"name":"stdout","output_type":"stream","text":["<re.Match object; span=(0, 3), match='asc'>\n"]}],"source":["print(r.search('asc'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nmGNRrQdQXD"},"outputs":[],"source":["r = re.compile('a*c')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u88K-P1Ud1eB","outputId":"77522b16-a1ae-4830-af15-95feed365575"},"outputs":[{"data":{"text/plain":["<re.Match object; span=(6, 7), match='c'>"]},"execution_count":19,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["r.search('aebsxecc')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEd6MXwOd4HA"},"outputs":[],"source":["r = re.compile('ab{2}c')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKTifcM_eut5","outputId":"45fa5a01-9618-42ef-80e9-bbb7795359b6"},"outputs":[{"data":{"text/plain":["<re.Match object; span=(0, 4), match='abbc'>"]},"execution_count":21,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["r.search('abbc')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eipZHpZyeykZ","outputId":"b82a6c94-59a6-4788-e13c-fcc6cd2c8ceb"},"outputs":[{"data":{"text/plain":["['정',\n"," '총리는',\n"," '이날',\n"," '정부서울청사에서',\n"," '주재한',\n"," '백신·치료제',\n"," '상황점검회의에서',\n"," '\"최근',\n"," '고령층에',\n"," '대한',\n"," 'AZ',\n"," '백신',\n"," '접종',\n"," '유보',\n"," '결정을',\n"," '계기로',\n"," '백신의',\n"," '안전성을',\n"," '우려하는',\n"," '목소리가',\n"," '있는',\n"," '걸로',\n"," '안다\"며',\n"," '이같이',\n"," '말했다.']"]},"execution_count":22,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["ex = '정 총리는 이날 정부서울청사에서 주재한 백신·치료제 상황점검회의에서 \"최근 고령층에 대한 AZ 백신 접종 유보 결정을 계기로 백신의 안전성을 우려하는 목소리가 있는 걸로 안다\"며 이같이 말했다.'\n","re.split(' ', ex)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z6TcFcqxgw7b","outputId":"c715607f-3dd7-4288-d9f6-eaafe9b78e1c"},"outputs":[{"data":{"text/plain":["['010', '1234', '1234', '30']"]},"execution_count":23,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["ex = '''이름 : 김철수\n","전화번호 : 010 - 1234 - 1234\n","나이 : 30\n","성별 : 남'''\n","\n","re.findall(\"\\d+\", ex)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kUGUgf2RhOB2","outputId":"fad4c699-2f33-487f-c182-ca25cafff0b2"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'정 총리는 \"50여 개국에서 승인을 받았고 며칠 전엔 세계보건기구(더블유에이치오)도 긴급사용승인을 했다\"며 \"접종이 시작된 국가들에서 심각한 부작용 사례도 보고된 적이 없다\"고 강조했다.'"]},"execution_count":24,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["ex = '정 총리는 \"50여 개국에서 승인을 받았고 며칠 전엔 세계보건기구(WHO)도 긴급사용승인을 했다\"며 \"접종이 시작된 국가들에서 심각한 부작용 사례도 보고된 적이 없다\"고 강조했다.'\n","\n","re.sub('WHO', '더블유에이치오', ex)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYVsmlG9h9ie"},"outputs":[],"source":["from nltk.tokenize import sent_tokenize\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3rw2NJEHq2G4"},"outputs":[],"source":["text = sent_tokenize(text)  # 문장 토큰화 진행"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SL4DuITVo1MP","outputId":"ff2348f0-a199-4fb6-e3d7-3653723a0ae2"},"outputs":[{"name":"stdout","output_type":"stream","text":["['the', 'barber', 'went', 'up', 'a', 'huge', 'mountain', '.', ['barber', 'went', 'huge', 'mountain']]\n"]}],"source":["# 문장으로 진행된 작업을 정제하여 단어 토큰화 수행\n","\n","vocab = {}\n","sentences = list()\n","stop_words = set(stopwords.words('english')) # 영어 불용어 기준으로 규정\n","\n","for i in text:\n","    sentences = word_tokenize(i)  # 단어 토큰화를 진행\n","    result = []\n","    # print(sentences)\n","    for word in sentences:\n","        # print(type(word), word)\n","        word = word.lower()  # 모든 단어를 소문자로 변환\n","        if word not in stop_words: # 불용어에 포함되지 않는 단어만 선별\n","            if len(word) > 2:  #길이가 짧은 단어 불포함\n","                result.append(word)\n","\n","                if word not in vocab:  # 단어를 vocab 사전에 하나씩 넣는다.\n","                    vocab[word] = 0   # 이때 동일한 단어가 없으면 0\n","                vocab[word] += 1      # 이미 해당 단어가 있으면 숫자를 1씩 증가\n","    sentences.append(result)\n","print(sentences)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wBZXCZ-xqaj5","outputId":"ca245cf4-d3e1-4f70-bb84-fed61c80a4fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"]}],"source":["print(vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Su6vfrNuZ3Q","outputId":"9e7eff18-3d80-4a64-85d4-b9d40535674d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"]}],"source":["vocab_sorted = sorted(vocab.items(), key= lambda x:x[1], reverse=True)\n","print(vocab_sorted)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iNiAIqYvvAJ9","outputId":"f8242788-9bb7-4b3c-dc6c-14fcbc6231f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"]}],"source":["word_to_index = {}\n","i = 0\n","for (word, frequency) in vocab_sorted:\n","    if frequency > 1:  # 빈도수가 적은 단어는 제외\n","        i +=1\n","        word_to_index[word] = i\n","print(word_to_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yLHTKwmp0dco","outputId":"ab0de8d9-f724-44dc-96b8-511ab36088ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"]}],"source":["vocab_size = 5\n","words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n","for w in words_frequency:\n","    del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n","print(word_to_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWXjIBR6vw_T"},"outputs":[],"source":["word_to_index['OOV'] = len(word_to_index)+1\n","# word_to_index의 제일 마지막 인덱스 번호를 부여한다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XBbMW_Dryh_c","outputId":"b2565989-e92f-455c-b8ed-845a5d9ac16d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[6, 6, 6], [6, 6, 6, 6, 6, 6], [6, 6, 6, 6], [6, 6], [6], [6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6], [6], [1, 6, 3, 6]]\n"]}],"source":["# 이제 word_to_index를 사용하여 sentences의 모든 단어들을 맵핑되는 정수로 인코딩한다.\n","\n","encoded = []\n","for s in sentences:\n","    temp = []\n","    for w in s :  # 정제 과정에서 삭제된 단어를 만날 경우 에러 발생시 처리를 위히여\n","        try:        # try~ except 구문을 사용한다.\n","            temp.append(word_to_index[w])\n","        except KeyError:\n","            temp.append(word_to_index['OOV'])\n","    encoded.append(temp)\n","print(encoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FG-M_udizGJV","outputId":"a3ce926c-092d-4483-9bd6-128e61e4a471"},"outputs":[{"data":{"text/plain":["['the',\n"," 'barber',\n"," 'went',\n"," 'up',\n"," 'a',\n"," 'huge',\n"," 'mountain',\n"," '.',\n"," ['barber', 'went', 'huge', 'mountain']]"]},"execution_count":34,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ai6Z60Cn0wN9"},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"na5G76Fn29_j"},"outputs":[],"source":["sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dN2TToAO3AUu","outputId":"61740c84-b2fd-42c2-da4b-1a2f0795c5f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"]}],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","# # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다.\n","\n","print(tokenizer.word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cr0aMlA93CVK","outputId":"f1066049-5485-4ab3-e658-a92f9b643b1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"]}],"source":["print(tokenizer.texts_to_sequences(sentences))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPpBeXAg3tfV","outputId":"ae992e50-7f27-4c35-877d-0ed96633b896"},"outputs":[{"name":"stdout","output_type":"stream","text":["단어 OOV 인덱스 : 1\n"]}],"source":["vocab_size = 5 # 단어 빈도수가 상위 5개의 단어만 사용\n","tokenizer = Tokenizer(num_words= vocab_size+2, oov_token='OOV')\n","# Tokenizer(num_words=숫자)는 빈도수가 높은 상위 몇 개의 단어만 사용하겠다고 지정\n","# 빈도수 높은 5개의 단어 + 패딩할때 생기는 인덱스 0 + oov 인덱스까지 계산\n","\n","tokenizer.fit_on_texts(sentences)\n","\n","print('단어 OOV 인덱스 : {}'.format(tokenizer.word_index['OOV']))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eUoBlimD5Y-E","outputId":"3bb73ac1-efb1-42a4-9de1-79e4eaafd440"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"]}],"source":["print(tokenizer.texts_to_sequences(sentences))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dRLkxJT75gt0","outputId":"ddb53096-9c57-4c02-ecba-72f825a16822"},"outputs":[{"data":{"text/plain":["array([[ 2,  6, 15, 15, 15],\n","       [ 2,  1,  6, 15, 15],\n","       [ 2,  4,  6, 15, 15],\n","       [ 1,  3, 15, 15, 15],\n","       [ 3,  5,  4,  3, 15],\n","       [ 4,  3, 15, 15, 15],\n","       [ 2,  5,  1, 15, 15],\n","       [ 2,  5,  1, 15, 15],\n","       [ 2,  5,  3, 15, 15],\n","       [ 4,  3,  1,  2,  1],\n","       [ 2,  1,  4,  1, 15]], dtype=int32)"]},"execution_count":41,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","encoded = tokenizer.texts_to_sequences(sentences)\n","last_value = len(tokenizer.word_index)+1\n","\n","padded = pad_sequences(encoded, padding='post', maxlen=5, value=last_value)\n","# maxlen= 으로 데이터의 크기를 조절했다.\n","# 빈 공간을 지정 값으로 대체하기 위해서 value= 인자를 사용한다.\n","\n","padded"]},{"cell_type":"markdown","metadata":{"id":"DlKknjIwUugU"},"source":["# 4. 카운트 기반의 단어 표현(Count based word Representation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ekdTl0y69Q3_"},"outputs":[],"source":["from konlpy.tag import Okt\n","import re\n","okt = Okt()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yST2q22PU7hm","outputId":"bfe058b3-47de-4a94-8471-629bbf2764e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n","[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"]}],"source":["token = re.sub(\"\\.\", \"\", \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n","# 마침표를 제거한다.\n","token = okt.morphs(token)\n","# morphemes(형태소) 분석기를 통해 토큰화한다.\n","\n","word2index={}  # 단어별 빈도수를 저장하기 위한 dict형태의 그릇을 만들고\n","bow=[]\n","for voca in token:\n","    if voca not in word2index.keys():\n","        word2index[voca]=len(word2index)\n","        # token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘어간다.\n","        bow.insert(len(word2index)-1, 1)\n","# bow리스트는 len(word2index)-1만큼의 개수이고, 각 1씩 배정한다. 최소 한번씩은 단어나 출연하기 때문\n","    else:\n","        index = word2index.get(voca) # 이미 word2index에 담긴 단어일 경우 기존 인덱스를 가져워\n","        bow[index] = bow[index]+1 # 출연 빈도에 +1을 해준다.\n","\n","print(word2index)\n","print(bow)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lGnWfhcwVjJU","outputId":"755fd6db-7c9c-4667-8d66-401585b7485f"},"outputs":[{"data":{"text/plain":["10"]},"execution_count":44,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["len(word2index)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GyEfDjUgXG_p","outputId":"764771a9-f669-4f54-d662-43edb83f38b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1 1 2 1 2 1]]\n","{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","corpus = ['you know I want your love. because I love you.']\n","vector = CountVectorizer()\n","print(vector.fit_transform(corpus).toarray())\n","print(vector.vocabulary_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uA8ctGoQeJ7j"},"outputs":[],"source":["import pandas as pd\n","from math import log"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DFFdQDhM3iMe","outputId":"7de15806-4819-4288-e93a-fc47d87ed492"},"outputs":[{"data":{"text/plain":["['과일이', '길고', '노란', '먹고', '바나나', '사과', '싶은', '저는', '좋아요']"]},"execution_count":47,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["docs = ['먹고 싶은 사과',\n","  '먹고 싶은 바나나',\n","  '길고 노란 바나나 바나나',\n","  '저는 과일이 좋아요' ]\n","\n","vocab = list(set(w for doc in docs for w in doc.split()))\n","# 전체 문서에서 각 문장을 doc로 담고, doc(각 문장)를 공백을 기준으로 나누어 w(word)로 반환 vocab에 담는다.\n","vocab.sort()\n","vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWHgMZcI3yCw"},"outputs":[],"source":["# TF, IDF, 그리고 TF-IDF 값을 구하는 함수를 구현한다.\n","\n","N = len(docs) # 총 문서의 수\n","\n","def tf(t, d):\n","    return d.count(t)\n","# 문서 d에서의 특정 단어 t의 등장 횟수\n","\n","def idf(t):\n","    df = 0\n","    for doc in docs:\n","        df += t in doc  # 특정단어 t가 문장에 있으면 +t등장 횟수\n","    return log(N/(df+1))  # df(t): 특정 단어 t가 등장한 문서의 수\n","\n","def tfidf(t, d):\n","    return tf(t,d)*idf(t)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"xeZ4RrwE50dn","outputId":"37f891a1-6d77-4ad9-e4bc-fdd597476fed"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>과일이</th>\n","      <th>길고</th>\n","      <th>노란</th>\n","      <th>먹고</th>\n","      <th>바나나</th>\n","      <th>사과</th>\n","      <th>싶은</th>\n","      <th>저는</th>\n","      <th>좋아요</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n","0    0   0   0   1    0   1   1   0    0\n","1    0   0   0   1    1   0   1   0    0\n","2    0   1   1   0    2   0   0   0    0\n","3    1   0   0   0    0   0   0   1    1"]},"execution_count":49,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# TF 구현\n","result=[]\n","for i in range(N):\n","    result.append([])\n","    d = docs[i]\n","    for j in range(len(vocab)):\n","        t = vocab[j]\n","        result[-1].append(tf(t,d))\n","\n","tf_ = pd.DataFrame(result, columns=vocab)\n","tf_"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"id":"TSWJ7JOd6bhn","outputId":"27768fde-85d3-40af-e91e-b495f9eabe09"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>IDF</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>과일이</th>\n","      <td>0.693147</td>\n","    </tr>\n","    <tr>\n","      <th>길고</th>\n","      <td>0.693147</td>\n","    </tr>\n","    <tr>\n","      <th>노란</th>\n","      <td>0.693147</td>\n","    </tr>\n","    <tr>\n","      <th>먹고</th>\n","      <td>0.287682</td>\n","    </tr>\n","    <tr>\n","      <th>바나나</th>\n","      <td>0.287682</td>\n","    </tr>\n","    <tr>\n","      <th>사과</th>\n","      <td>0.693147</td>\n","    </tr>\n","    <tr>\n","      <th>싶은</th>\n","      <td>0.287682</td>\n","    </tr>\n","    <tr>\n","      <th>저는</th>\n","      <td>0.693147</td>\n","    </tr>\n","    <tr>\n","      <th>좋아요</th>\n","      <td>0.693147</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          IDF\n","과일이  0.693147\n","길고   0.693147\n","노란   0.693147\n","먹고   0.287682\n","바나나  0.287682\n","사과   0.693147\n","싶은   0.287682\n","저는   0.693147\n","좋아요  0.693147"]},"execution_count":50,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# IDF 구하기\n","\n","result = []\n","for j in range(len(vocab)):\n","    t = vocab[j]\n","    result.append(idf(t))\n","\n","idf_ = pd.DataFrame(result, index=vocab, columns=['IDF'])\n","idf_"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"AH8WeQ_47gsK","outputId":"7115bd64-98c2-48b3-f544-7c08d188aac4"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>과일이</th>\n","      <th>길고</th>\n","      <th>노란</th>\n","      <th>먹고</th>\n","      <th>바나나</th>\n","      <th>사과</th>\n","      <th>싶은</th>\n","      <th>저는</th>\n","      <th>좋아요</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.287682</td>\n","      <td>0.000000</td>\n","      <td>0.693147</td>\n","      <td>0.287682</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.287682</td>\n","      <td>0.287682</td>\n","      <td>0.000000</td>\n","      <td>0.287682</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>0.693147</td>\n","      <td>0.693147</td>\n","      <td>0.000000</td>\n","      <td>0.575364</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.693147</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.693147</td>\n","      <td>0.693147</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        과일이        길고        노란  ...        싶은        저는       좋아요\n","0  0.000000  0.000000  0.000000  ...  0.287682  0.000000  0.000000\n","1  0.000000  0.000000  0.000000  ...  0.287682  0.000000  0.000000\n","2  0.000000  0.693147  0.693147  ...  0.000000  0.000000  0.000000\n","3  0.693147  0.000000  0.000000  ...  0.000000  0.693147  0.693147\n","\n","[4 rows x 9 columns]"]},"execution_count":51,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# TF-IDF 행렬 출력\n","\n","result =[]\n","for i in range(N):\n","    result.append([])\n","    d = docs[i]\n","    for j in range(len(vocab)):\n","        t = vocab[j]\n","\n","        result[-1].append(tfidf(t,d))\n","\n","tfidf_ = pd.DataFrame(result, columns=vocab)\n","tfidf_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fF0L0wR-8IdV"},"outputs":[],"source":["from numpy import dot\n","from numpy.linalg import norm\n","import numpy as np\n","\n","def cos_sim(A, B):\n","    return dot(A, B)/(norm(A)*norm(B))\n","\n","def cos_similarity(v1, v2):\n","    dot_product = np.dot(v1, v2) #두 벡터에 내적 한다.\n","    l2_norm = (np.sqrt(sum(np.square(v1))))*np.sqrt(sum(np.square(v2)))\n","    similarity = dot_product /l2_norm\n","\n","    return similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpbC2CKMATC0"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1d7b8_GXGeMQ"},"source":["# 5. 벡터의 유사도(Vector Similarity)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5wuDK9qGfXA"},"outputs":[],"source":["# 코사인 유사도를 구하기\n","from numpy import dot\n","from numpy.linalg import norm\n","import numpy as np\n","def cos_sim(A, B):\n","    return dot(A,B)/(norm(A)*norm(B))\n","\n","\n","# 혹은 이렇게도 구현할 수 있다.\n","def cos_similarity(v1, v2):\n","    dot_product = np.dot(v1, v2)\n","    l2_norm = (np.sqrt(sum(np.square(v1)))) * np.sqrt(sum(np.square(v2)))\n","    similarity = dot_product / l2_norm\n","    # np.square()는 배열 원소의 제곱값 (square value)을 구하는 함수\n","\n","    return similarity"]},{"cell_type":"markdown","metadata":{"id":"_-J5N8gTa3qH"},"source":["# 6. 토픽 모델링(Topic Modeling)\n","\n","### 1) LSA 실습"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-smu-C2GwUz","outputId":"934e923b-f34a-4c77-f623-13fac4140290"},"outputs":[{"data":{"text/plain":["11314"]},"execution_count":2,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# 사이킷런 Twenty Newsgroups을 사용한 실습\n","# 뉴스그룹 데이터는 뉴스 데이터가 아님\n","# LSA를 사용해서 문서의 수를 원하는 토픽의 수로 압축한 뒤에 각 토픽당 가장 중요한 단어 5개를 출력하는 실습 수행\n","\n","import pandas as pd\n","from sklearn.datasets import fetch_20newsgroups\n","\n","dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n","documents = dataset.data\n","len(documents)  # 훈련용 데이터 11314 출력"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"klJnRC_waxS5","outputId":"56aa7756-3937-4c6f-c029-2cb37167f78c"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""]},"execution_count":3,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["documents[1]\n","# 뉴스그룹 데이터에는 특수문자가 포함된 다수의 영어문장으로 구성되어져 있다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TvC5QHv9b0hY","outputId":"d335f78c-d037-4572-f883-2ec19d23aecc"},"outputs":[{"name":"stdout","output_type":"stream","text":["['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"]}],"source":["# target_name에는 본래 이 뉴스그룹 데이터가 어떤 20개의 카테고리를 갖고있었는지가 저장되어져 있다.\n","print(dataset.target_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EExMG3dIcJHw"},"outputs":[],"source":["# 텍스트 전처리 수행\n","\n","news_df = pd.DataFrame({'document':documents})\n","news_df['clean_doc'] = news_df['document'].str.replace('[^a-zA-Z]',' ')\n","# 특수 문자를 제거하고 알파벳만 담는다.\n","\n","news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n","# 길이가 3이하인 단어는 제거\n","\n","news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"fw972Ad9hXzV","outputId":"c49bb939-b332-4d31-a9cf-0afb4d7a4585"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"]},"execution_count":6,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["news_df['clean_doc'][1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YI2DQrx3hh_-","outputId":"e98d0ebf-6dd7-4633-8869-cf6884207a1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# 토큰화 진행 및 불용어 제거\n","import nltk\n","nltk.download('stopwords')\n","\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아온다.\n","tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n","tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) # 불용어 제거"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKQuj7UciUFC","outputId":"548e1ab0-a00c-4408-db0c-2da30acd839a"},"outputs":[{"name":"stdout","output_type":"stream","text":["['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons']\n"]}],"source":["print(tokenized_doc[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B05toEbsiwq9","outputId":"c1f82075-c1ec-4d1d-a67a-d6dcfe05a076"},"outputs":[{"name":"stdout","output_type":"stream","text":["yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons\n"]}],"source":["# tf-idf 행렬 만들기\n","# TfidfVectorizer는 토큰화가 되어있지 않은 텍스트 데이터를 입력으로 사용하기 때문에 역 토큰화 작업을 수행한다.\n","\n","# 토큰화 취소 작업\n","detokenized_doc = []\n","for i in range(len(news_df)):\n","    t = ' '.join(tokenized_doc[i])\n","    detokenized_doc.append(t)\n","\n","news_df['clean_doc'] = detokenized_doc\n","\n","print(news_df['clean_doc'][1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tM5NWqLojk__","outputId":"adec4edb-1e9f-4e78-c14e-01db846669f7"},"outputs":[{"data":{"text/plain":["(11314, 1000)"]},"execution_count":10,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# 상위 1000개의 단어만 TF-IDF 행렬 생성\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, max_df=0.5, smooth_idf=True)\n","X = vectorizer.fit_transform(news_df['clean_doc'])\n","X.shape  #(11314, 1000) 출력 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EGiH67JgkXsp","outputId":"fb349ea9-e7b9-4678-c948-245af2a3b5d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["20\n","(20, 1000)\n"]}],"source":["# 토픽 모델링 진행\n","import numpy as np\n","from sklearn.decomposition import TruncatedSVD\n","\n","svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n","# 토픽의 숫자 n_components 파라미터로 지정\n","svd_model.fit(X)\n","print(len(svd_model.components_)) # 20\n","# svd_model.componets_는 앞서 배운 LSA에서 V^T에 해당\n","print(np.shape(svd_model.components_)) #(20, 1000)\n","# 정확하게 토픽의 수 t × 단어의 수의 크기를 가진다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"idlNUxk4lUI2","outputId":"4b942730-ae77-4e26-b425-1e6b2b2fb0aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Topic 1: [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n","Topic 2: [('thanks', 0.32888), ('windows', 0.29088), ('card', 0.18069), ('drive', 0.17455), ('mail', 0.15111)]\n","Topic 3: [('game', 0.37064), ('team', 0.32443), ('year', 0.28154), ('games', 0.2537), ('season', 0.18419)]\n","Topic 4: [('drive', 0.53324), ('scsi', 0.20165), ('hard', 0.15628), ('disk', 0.15578), ('card', 0.13994)]\n","Topic 5: [('windows', 0.40399), ('file', 0.25436), ('window', 0.18044), ('files', 0.16078), ('program', 0.13894)]\n","Topic 6: [('chip', 0.16114), ('government', 0.16009), ('mail', 0.15625), ('space', 0.1507), ('information', 0.13562)]\n","Topic 7: [('like', 0.67086), ('bike', 0.14236), ('chip', 0.11169), ('know', 0.11139), ('sounds', 0.10371)]\n","Topic 8: [('card', 0.46633), ('video', 0.22137), ('sale', 0.21266), ('monitor', 0.15463), ('offer', 0.14643)]\n","Topic 9: [('know', 0.46047), ('card', 0.33605), ('chip', 0.17558), ('government', 0.1522), ('video', 0.14356)]\n","Topic 10: [('good', 0.42756), ('know', 0.23039), ('time', 0.1882), ('bike', 0.11406), ('jesus', 0.09027)]\n","Topic 11: [('think', 0.78469), ('chip', 0.10899), ('good', 0.10635), ('thanks', 0.09123), ('clipper', 0.07946)]\n","Topic 12: [('thanks', 0.36824), ('good', 0.22729), ('right', 0.21559), ('bike', 0.21037), ('problem', 0.20894)]\n","Topic 13: [('good', 0.36212), ('people', 0.33985), ('windows', 0.28385), ('know', 0.26232), ('file', 0.18422)]\n","Topic 14: [('space', 0.39946), ('think', 0.23258), ('know', 0.18074), ('nasa', 0.15174), ('problem', 0.12957)]\n","Topic 15: [('space', 0.31613), ('good', 0.3094), ('card', 0.22603), ('people', 0.17476), ('time', 0.14496)]\n","Topic 16: [('people', 0.48156), ('problem', 0.19961), ('window', 0.15281), ('time', 0.14664), ('game', 0.12871)]\n","Topic 17: [('time', 0.34465), ('bike', 0.27303), ('right', 0.25557), ('windows', 0.1997), ('file', 0.19118)]\n","Topic 18: [('time', 0.5973), ('problem', 0.15504), ('file', 0.14956), ('think', 0.12847), ('israel', 0.10903)]\n","Topic 19: [('file', 0.44163), ('need', 0.26633), ('card', 0.18388), ('files', 0.17453), ('right', 0.15448)]\n","Topic 20: [('problem', 0.33006), ('file', 0.27651), ('thanks', 0.23578), ('used', 0.19206), ('space', 0.13185)]\n"]}],"source":["terms = vectorizer.get_feature_names() # 위에서 저장한 1000의 단어 terms에 담는다.\n","\n","def get_topics(components, feature_names, n=5):\n","    for idx, topic in enumerate(components):\n","        print('Topic %d:' % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n-1:-1]])\n","        # arg를 정렬해서 역으로 5개를 뽑는다. 즉, [:-6]를 뒤에서부터 세어서 출력한다.\n","        # 각 20개의 행의 각 1,000개의 열 중 가장 값이 큰 5개의 값을 찾아서 단어로 출력한다.\n","get_topics(svd_model.components_, terms)"]},{"cell_type":"markdown","metadata":{"id":"tASHgwgRF3U3"},"source":["### 2) LDA 실습하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLTL6z2-nehG","outputId":"d09be5b9-4ac9-4a2c-b965-600e827c1a39"},"outputs":[{"data":{"text/plain":["0    [well, sure, story, seem, biased, disagree, st...\n","1    [yeah, expect, people, read, actually, accept,...\n","2    [although, realize, principle, strongest, poin...\n","3    [notwithstanding, legitimate, fuss, proposal, ...\n","4    [well, change, scoring, playoff, pool, unfortu...\n","Name: clean_doc, dtype: object"]},"execution_count":14,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# LSA 챕터에서 사용하였던 Twenty Newsgroups 사용\n","# 전처리 과정을 거친 tokenized_doc을 그대로 사용한다.\n","tokenized_doc[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"idiUAGReoOOw","outputId":"3f15b5a6-c4c3-4e36-e641-4c766dcf5a38"},"outputs":[{"name":"stdout","output_type":"stream","text":["[(52, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 2), (86, 1), (87, 1), (88, 1), (89, 1)]\n"]}],"source":["# 1) 정수 인코딩과 단어 집합 만들기\n","\n","from gensim import corpora\n","dictionary = corpora.Dictionary(tokenized_doc)\n","corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n","# 각 단어를 (word_id, word_frequency)의 형태로 출력\n","# word_id는 단어가 정수 인코딩된 값이고, word_frequency는 해당 뉴스에서의 해당 단어의 빈도수를 의미\n","print(corpus[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RqJqJeDEHe1t","outputId":"c099aa5d-70fc-408f-e276-6a468370019c"},"outputs":[{"name":"stdout","output_type":"stream","text":["faith\n","64281\n"]}],"source":["# 정수 인코딩되기 이전의 기존 단어 확인\n","\n","print(dictionary[66]) # 66으로 인코딩된 단어는 faith 였음\n","print(len(dictionary)) # 총 학습된 단어의 개수 확인 # 결과 : 64281"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9pVfE9gMILYD","outputId":"c8b55f53-6c02-4779-e320-20a06bf4e5ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["(0, '0.013*\"information\" + 0.010*\"mail\" + 0.009*\"available\" + 0.008*\"data\"')\n","(1, '0.007*\"april\" + 0.006*\"germany\" + 0.006*\"navy\" + 0.005*\"david\"')\n","(2, '0.027*\"space\" + 0.007*\"launch\" + 0.007*\"nasa\" + 0.007*\"earth\"')\n","(3, '0.010*\"nrhj\" + 0.006*\"wwiz\" + 0.006*\"bxom\" + 0.006*\"gizw\"')\n","(4, '0.042*\"drive\" + 0.026*\"scsi\" + 0.024*\"card\" + 0.024*\"disk\"')\n","(5, '0.010*\"people\" + 0.009*\"would\" + 0.006*\"jesus\" + 0.006*\"think\"')\n","(6, '0.014*\"windows\" + 0.012*\"file\" + 0.009*\"window\" + 0.009*\"files\"')\n","(7, '0.017*\"would\" + 0.011*\"people\" + 0.007*\"time\" + 0.006*\"even\"')\n","(8, '0.026*\"entry\" + 0.026*\"output\" + 0.020*\"file\" + 0.015*\"program\"')\n","(9, '0.009*\"said\" + 0.008*\"people\" + 0.008*\"know\" + 0.007*\"like\"')\n","(10, '0.020*\"period\" + 0.013*\"play\" + 0.012*\"power\" + 0.008*\"goal\"')\n","(11, '0.011*\"health\" + 0.009*\"medical\" + 0.008*\"study\" + 0.007*\"university\"')\n","(12, '0.014*\"government\" + 0.008*\"public\" + 0.008*\"encryption\" + 0.008*\"state\"')\n","(13, '0.015*\"chip\" + 0.008*\"chips\" + 0.007*\"number\" + 0.007*\"data\"')\n","(14, '0.021*\"game\" + 0.020*\"team\" + 0.015*\"year\" + 0.014*\"games\"')\n","(15, '0.018*\"armenian\" + 0.014*\"turkish\" + 0.014*\"armenians\" + 0.009*\"jews\"')\n","(16, '0.018*\"sale\" + 0.014*\"shipping\" + 0.013*\"offer\" + 0.012*\"condition\"')\n","(17, '0.010*\"president\" + 0.010*\"israel\" + 0.007*\"people\" + 0.007*\"would\"')\n","(18, '0.017*\"would\" + 0.014*\"like\" + 0.011*\"good\" + 0.010*\"know\"')\n","(19, '0.014*\"ground\" + 0.013*\"wire\" + 0.009*\"pain\" + 0.008*\"wiring\"')\n"]}],"source":["# 2) LDA 모델 훈련시키기\n","\n","import gensim\n","NUM_TOPICS = 20 # k=20, 토픽 갯수\n","ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n","# passes는 알고리즘의 동작 횟수\n","\n","# num_words=4로 총 4개의 단어만 출력\n","topics = ldamodel.print_topics(num_words=4)\n","for topic in topics:\n","    print(topic)\n","\n","# 출력 결과의 각 단어 앞에 붙은 수치는 단어의 해당 토픽에 대한 기여도를 보여준다."]},{"cell_type":"markdown","metadata":{"id":"Nappzd0zeVu0"},"source":["### 3) LDA 시각화 하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"chil-H1nJmzH","outputId":"1958f34f-c12c-409b-e45c-9a5d098e4c6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.7/dist-packages (3.3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.20.2)\n","Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.15)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.22.2.post1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.2.4)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (56.0.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (1.1.1)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.0.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n"]}],"source":["!pip install pyLDAvis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mt2t1IGXKjlb"},"outputs":[],"source":["import pyLDAvis.gensim\n","pyLDAvis.enable_notebook()\n","vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n","pyLDAvis.display(vis)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yo6-URiMLB_o","outputId":"d917c129-f993-40f1-afd7-29272d7486b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 번째 문서의 토픽 비율은 [(1, 0.02106845), (4, 0.11156815), (5, 0.47271356), (7, 0.121322095), (14, 0.022925178), (15, 0.15421031), (17, 0.085708365)]\n","1 번째 문서의 토픽 비율은 [(2, 0.025716398), (4, 0.05553543), (5, 0.37832165), (13, 0.22192067), (18, 0.30021316)]\n","2 번째 문서의 토픽 비율은 [(7, 0.36074966), (10, 0.016935483), (12, 0.021234037), (17, 0.58817756)]\n","3 번째 문서의 토픽 비율은 [(4, 0.012050203), (7, 0.44534928), (12, 0.3098471), (17, 0.023012893), (18, 0.19854656)]\n","4 번째 문서의 토픽 비율은 [(8, 0.045858104), (9, 0.23605196), (14, 0.42962876), (18, 0.25883153)]\n"]}],"source":["# 4) 문서 별 토픽 분포 보기\n","# 각 문서의 토픽 분포는 이미 훈련된 LDA 모델인 ldamodel[]에 전체 데이터가 정수 인코딩 된 결과를 넣은 후에 확인이 가능하다.\n","\n","for i, topic_list in enumerate(ldamodel[corpus]):\n","    if i == 5:  # 전체 문서에 확인 가능하나, 5개의 문서만 수행하고 중지한다.\n","        break\n","    print(i, '번째 문서의 토픽 비율은', topic_list)\n","\n","    # 출력 결과를 살펴보면, 0번째 문서의 토픽 비율에서 (1, 0.15287682)은 1번 토픽이 15%의 분포도를 가지는 것을 의미"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXResTbYMEPa"},"outputs":[],"source":["def make_topictable_per_doc(ldamodel, corpus):\n","    topic_table = pd.DataFrame()\n","\n","    for i, topic_list in enumerate(ldamodel[corpus]):\n","        doc = topic_list[0] if ldamodel.per_word_topics else topic_list\n","        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n","\n","        for j, (topic_num, prop_topic) in enumerate(doc):\n","            if j == 0:\n","                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4),topic_list]), ignore_index=True)\n","            else:\n","                break\n","    return(topic_table)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"woBqjek-Ofyj"},"outputs":[],"source":["topictable = make_topictable_per_doc(ldamodel, corpus)\n","topictable = topictable.reset_index()\n","topictable.columns = ['문서 번호', '가장 비중이 높은 토픽 번호', '가장 높은 토픽의 비중', '각 토픽의 비중']\n","topictable[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dSa6xzOGiQG1"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"NLP_base01.ipynb","provenance":[{"file_id":"https://gist.github.com/Lucia-KIM/9291bc240bddfc2ffd53ca5d1d92eba9#file-nlp_base01-ipynb","timestamp":1689146818506}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
