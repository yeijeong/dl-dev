{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import ssl\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "def makeUrl(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to):\n",
    "    urls = []\n",
    "    if page == maxpage_t:       # 최대페이지 1일 경우\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        urls.append(url)\n",
    "    else:\n",
    "        while page <= maxpage_t :\n",
    "            url = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "            page += 10\n",
    "    return urls    \n",
    "\n",
    "# html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "def news_attrs_crawler(articles,attrs):\n",
    "    attrs_content=[]\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "\n",
    "#html생성해서 기사크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    #html 불러오기\n",
    "    original_html = requests.get(url, headers=headers, verify=False)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "\n",
    "    url_naver = html.select(\"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver,'href')\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 119837.26it/s]\n"
     ]
    }
   ],
   "source": [
    "#####뉴스크롤링 시작#####\n",
    "\n",
    "# 최대 페이지 수\n",
    "maxpage = int(input(\"최대 크롤링할 페이지 수 입력하시오: \"))\n",
    "#검색어 입력\n",
    "search = input(\"검색할 키워드를 입력해주세요:\")\n",
    "# 검색 방식\n",
    "sort = input(\"뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): \")    #관련도순=0  최신순=1  오래된순=2\n",
    "# 시작날짜\n",
    "s_date = input(\"시작날짜 입력(2019.01.04):\")  #2019.01.04\n",
    "# 끝나는 날짜\n",
    "e_date = input(\"끝날짜 입력(2019.01.05):\")   #2019.01.05\n",
    "\n",
    "# 날짜 바꾸기\n",
    "s_from = s_date.replace(\".\",\"\")\n",
    "e_to = e_date.replace(\".\",\"\")\n",
    "\n",
    "page = 1  \n",
    "maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "# naver url 생성\n",
    "urls = makeUrl(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "\n",
    "#뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_url =[]\n",
    "news_contents =[]\n",
    "news_dates = []\n",
    "\n",
    "# for i in urls:\n",
    "#     url = articles_crawler(urls)\n",
    "#     news_url.append(url)\n",
    "\n",
    "while page <= maxpage_t :\n",
    "    url_origin = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "    url = articles_crawler(url_origin)\n",
    "    news_url .append(url)\n",
    "    page += 10\n",
    "\n",
    "#제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "def makeList(newlist, content):\n",
    "    for i in content:\n",
    "        for j in i:\n",
    "            newlist.append(j)\n",
    "    return newlist\n",
    "\n",
    "#제목, 링크, 내용 담을 리스트 생성\n",
    "news_url_1 = []\n",
    "\n",
    "#1차원 리스트로 만들기(내용 제외)\n",
    "makeList(news_url_1,news_url)\n",
    "\n",
    "#NAVER 뉴스만 남기기\n",
    "final_urls = []\n",
    "for i in tqdm(range(len(news_url_1))):\n",
    "    if \"news.naver.com\" in news_url_1[i]:\n",
    "        final_urls.append(news_url_1[i])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 내용 크롤링\n",
    "\n",
    "for i in tqdm(final_urls):\n",
    "    #각 기사 html get하기\n",
    "    headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "    news = requests.get(i, headers=headers, verify=False)\n",
    "    news_html = BeautifulSoup(news.text,\"html.parser\")\n",
    "\n",
    "    # 뉴스 제목 가져오기\n",
    "    title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    if title == None:\n",
    "        title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "    \n",
    "    # 뉴스 본문 가져오기\n",
    "    content = news_html.select(\"div#dic_area\")\n",
    "    if content == []:\n",
    "        content = news_html.select(\"#articeBody\")\n",
    "        \n",
    "    # 기사 텍스트만 가져오기\n",
    "    # list합치기\n",
    "    content = ''.join(str(content))\n",
    "\n",
    "    # html태그제거 및 텍스트 다듬기\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "    content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "    pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "    content = content.replace(pattern2, '')\n",
    "\n",
    "    news_titles.append(title)\n",
    "    news_contents.append(content)\n",
    "\n",
    "    # try:\n",
    "    #     html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "    #     news_date = html_date.attrs['data-date-time']\n",
    "    # except AttributeError:\n",
    "    #     news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "    #     news_date = re.sub(pattern=pattern1,repl='',string=str(news_date))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# 날짜 가져오기\n",
    "for i in tqdm(final_urls):\n",
    "    #각 기사 html get하기\n",
    "    headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "    news = requests.get(i, headers=headers, verify=False)\n",
    "    news_html = BeautifulSoup(news.text,\"html.parser\")\n",
    "    \n",
    "    html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "    news_date = html_date.attrs['data-date-time']\n",
    "    news_dates.append(news_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-07-01 07:01:17'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['상반기 결산해보니…개미 2차전지·외국인 삼성전자 담았다', '상반기 결산해보니…개미 2차전지·외국인 삼성전자 담았다']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://n.news.naver.com/mnews/article/003/0011947048?sid=101',\n",
       " 'https://n.news.naver.com/mnews/article/003/0011947048?sid=101']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[\\n2차전지 열풍 에코프로 593.2% 급등 외국인 러브콜 삼성전자 30.92% 올라\\n\\n\\n\\n[서울=뉴시스] 고승민 기자 = 삼성전자가 2022년 4분기 실적을 발표한 31일 서울 삼성전자 서초사옥 모습. 삼성전자에 따르면 31일 연결 기준으로 2022년 4분기 매출은 전년 76조5655억원 대비 8% 감소했고, 영업이익도 전년 4분기 13조8668억원보다 69% 줄었다. 올 1분기 역시 글로벌 IT 수요 부진과 반도체 시황 약세가 지속될 것으로 예상되면서 하반기에나 수요 회복을 기대할 수 있다는 전망이다. 2023.01.31. kkssmm99@newsis.com[서울=뉴시스] 강수윤 기자 = 올 상반기 국내 증시에서 개인은 2차전지를, 외국인은 삼성전자 등 반도체주를 쓸어담은 것으로 나타났다. 1일 한국거래소에 따르면 올해 1월2일부터 지난 달 29일까지 개인투자자들이 가장 많이 순매수한 종목은 2차전지 관련주였다. 개인투자자들의 순매수 상위 10개 종목은 POSCO홀딩스(4조7371억원), 에코프로(1조9643억원), 에코프로비엠(1조1198억원), 카카오(5597억원), SK이노베이션(5490억원), 네이버(5114억원), LG화학(4798억원), LG생활건강(4759억원), 한화솔루션(4685억원), 포스코퓨처엠(3744억원) 등의 순이었다. 개인 투자자는 올 들어 급등했던 2차전지주에서 높은 수익을 냈다. 2차전지는 전기차 보급 확대에 힘입어 상반기 주식시장의 열풍을 일으키며 핵심 테마로 떠올랐다. 특히 2차전지 소재 대장주 에코프로와 에코프로비엠은 각각 593.2%, 161.67%나 폭등했다. '에코프로 삼형제'라 불리는 에코프로·에코프로비엠·에코프로에이치엔의 활약에 코스닥 지수는 11개월여 만에 900선을 돌파하기도 했다. 코스피시장에서도 POSCO홀딩스(40.14%) SK이노베이션(3.32%), LG화학(10%), 포스코퓨처엠(94.44%) 등이 일제히 상승하며 2차전지주저력을 과시했다. 2차전지에서 돈을 번 개미들은 LG생활건강(-36.08%), 한화솔루션(-22.17%), 카카오(-7.72%) 등 빅테크주와 화장품주에서 수익을 까먹었다. 네이버도 3.32% 수익에 그쳤다. 반면 올해 증시 상승을 이끈 외국인 투자자들은 반도체주를 가장 많이 사들였다. 국내 반도체 대형주인 삼성전자와 SK하이닉스는 외국인이 올 들어 각각 12조372억원, 1조5504억원 어치 사들이며 나란히 순매수 종목 1, 2위에 올랐다. 외국인의 자금 유입으로 삼성전자와 SK하이닉스 주가는 30.92%, 54.93%나 올랐다. 주력사업인 메모리 반도체 업황이 바닥을 지난 데다, 엔비디아발 인공지능(AI) 열풍의 수혜를 받을 것이란 전망에서다. AI 서버 개발에 필요한 반도체  HBM 시장은 삼성전자와 SK하이닉스가 주도하고 있다.외국인은 자동차주와 엔터주도 열심히 담았다. 외국인 투자자는 현대차와 기아 주식을 각각 1조4216억원, 5849억원을 순매수했다. 이에 따라 주가는 35.43%, 45.02% 상승했다. 1분기 실적에 이어 올해 영업이익이 삼성그룹을 넘어설 것이란 관측이 나오면서 외국인 투자자들이 현대차그룹 계열사 주식을 적극적으로 사들이고 있다.K-팝 글로벌 인기에 엔터주도 연일 신고가를 기록하며 강세를 보였다. 외국인이 4169억원 어치를 사들인 JYP엔터는 순매수 상위 종목 10위에 올랐다. 주가가 84.12%나 폭등하며 상위 종목 가운데 가장 높은 상승률을 기록했다. JYP엔터는 올 초만 해도 코스닥 상장사 중 시총 규모 10위에 머물렀으나 5위로 뛰어오르며 5조원 돌파를 눈 앞에 두고 있다. 외국인은 이밖에 삼성SDI(9041억원·12.52%), LG전자(6853억원·45.66%), 한화에어로스페이스(6096억원·70.1%), 현대로템(4472억원·25.52%), LG에너지솔루션(4205억원·26.29%) 등을 순매수했다.한편 개인투자자는 최근 삼성전자 등 반도체주가 오르기 내다 팔고 있다. 개인은 상반기에만 삼성전자 주식 1조5126억원어치를 순매도했다.  5~6만원대에 머물던 주가가 1년2개월 만에 '7만전자'를 회복하자 차익실현을 위해 매도에 나선 것으로 분석된다．\\n]\",\n",
       " \"[\\n2차전지 열풍 에코프로 593.2% 급등 외국인 러브콜 삼성전자 30.92% 올라\\n\\n\\n\\n[서울=뉴시스] 고승민 기자 = 삼성전자가 2022년 4분기 실적을 발표한 31일 서울 삼성전자 서초사옥 모습. 삼성전자에 따르면 31일 연결 기준으로 2022년 4분기 매출은 전년 76조5655억원 대비 8% 감소했고, 영업이익도 전년 4분기 13조8668억원보다 69% 줄었다. 올 1분기 역시 글로벌 IT 수요 부진과 반도체 시황 약세가 지속될 것으로 예상되면서 하반기에나 수요 회복을 기대할 수 있다는 전망이다. 2023.01.31. kkssmm99@newsis.com[서울=뉴시스] 강수윤 기자 = 올 상반기 국내 증시에서 개인은 2차전지를, 외국인은 삼성전자 등 반도체주를 쓸어담은 것으로 나타났다. 1일 한국거래소에 따르면 올해 1월2일부터 지난 달 29일까지 개인투자자들이 가장 많이 순매수한 종목은 2차전지 관련주였다. 개인투자자들의 순매수 상위 10개 종목은 POSCO홀딩스(4조7371억원), 에코프로(1조9643억원), 에코프로비엠(1조1198억원), 카카오(5597억원), SK이노베이션(5490억원), 네이버(5114억원), LG화학(4798억원), LG생활건강(4759억원), 한화솔루션(4685억원), 포스코퓨처엠(3744억원) 등의 순이었다. 개인 투자자는 올 들어 급등했던 2차전지주에서 높은 수익을 냈다. 2차전지는 전기차 보급 확대에 힘입어 상반기 주식시장의 열풍을 일으키며 핵심 테마로 떠올랐다. 특히 2차전지 소재 대장주 에코프로와 에코프로비엠은 각각 593.2%, 161.67%나 폭등했다. '에코프로 삼형제'라 불리는 에코프로·에코프로비엠·에코프로에이치엔의 활약에 코스닥 지수는 11개월여 만에 900선을 돌파하기도 했다. 코스피시장에서도 POSCO홀딩스(40.14%) SK이노베이션(3.32%), LG화학(10%), 포스코퓨처엠(94.44%) 등이 일제히 상승하며 2차전지주저력을 과시했다. 2차전지에서 돈을 번 개미들은 LG생활건강(-36.08%), 한화솔루션(-22.17%), 카카오(-7.72%) 등 빅테크주와 화장품주에서 수익을 까먹었다. 네이버도 3.32% 수익에 그쳤다. 반면 올해 증시 상승을 이끈 외국인 투자자들은 반도체주를 가장 많이 사들였다. 국내 반도체 대형주인 삼성전자와 SK하이닉스는 외국인이 올 들어 각각 12조372억원, 1조5504억원 어치 사들이며 나란히 순매수 종목 1, 2위에 올랐다. 외국인의 자금 유입으로 삼성전자와 SK하이닉스 주가는 30.92%, 54.93%나 올랐다. 주력사업인 메모리 반도체 업황이 바닥을 지난 데다, 엔비디아발 인공지능(AI) 열풍의 수혜를 받을 것이란 전망에서다. AI 서버 개발에 필요한 반도체  HBM 시장은 삼성전자와 SK하이닉스가 주도하고 있다.외국인은 자동차주와 엔터주도 열심히 담았다. 외국인 투자자는 현대차와 기아 주식을 각각 1조4216억원, 5849억원을 순매수했다. 이에 따라 주가는 35.43%, 45.02% 상승했다. 1분기 실적에 이어 올해 영업이익이 삼성그룹을 넘어설 것이란 관측이 나오면서 외국인 투자자들이 현대차그룹 계열사 주식을 적극적으로 사들이고 있다.K-팝 글로벌 인기에 엔터주도 연일 신고가를 기록하며 강세를 보였다. 외국인이 4169억원 어치를 사들인 JYP엔터는 순매수 상위 종목 10위에 올랐다. 주가가 84.12%나 폭등하며 상위 종목 가운데 가장 높은 상승률을 기록했다. JYP엔터는 올 초만 해도 코스닥 상장사 중 시총 규모 10위에 머물렀으나 5위로 뛰어오르며 5조원 돌파를 눈 앞에 두고 있다. 외국인은 이밖에 삼성SDI(9041억원·12.52%), LG전자(6853억원·45.66%), 한화에어로스페이스(6096억원·70.1%), 현대로템(4472억원·25.52%), LG에너지솔루션(4205억원·26.29%) 등을 순매수했다.한편 개인투자자는 최근 삼성전자 등 반도체주가 오르기 내다 팔고 있다. 개인은 상반기에만 삼성전자 주식 1조5126억원어치를 순매도했다.  5~6만원대에 머물던 주가가 1년2개월 만에 '7만전자'를 회복하자 차익실현을 위해 매도에 나선 것으로 분석된다．\\n]\"]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 후 행 개수:  1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'datetime.datetime' has no attribute 'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m중복 제거 후 행 개수: \u001b[39m\u001b[39m\"\u001b[39m,\u001b[39mlen\u001b[39m(news_df))\n\u001b[1;32m     12\u001b[0m \u001b[39m#데이터 프레임 저장\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m now \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39;49mdatetime\u001b[39m.\u001b[39mnow() \n\u001b[1;32m     14\u001b[0m news_df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39m//Users//yeijeong//big16//final_project//DATANEWS_DATA//현대로템//\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_page\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(search,now\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m),page,maxpage_t),encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8-sig\u001b[39m\u001b[39m'\u001b[39m,index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'datetime.datetime' has no attribute 'datetime'"
     ]
    }
   ],
   "source": [
    "###데이터 프레임으로 만들기###\n",
    "import pandas as pd\n",
    "\n",
    "#데이터 프레임 만들기\n",
    "news_df = pd.DataFrame({'date':news_dates,'title':news_titles,'link':final_urls,'content':news_contents})\n",
    "news_df\n",
    "\n",
    "#중복 행 지우기\n",
    "news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "print(\"중복 제거 후 행 개수: \",len(news_df))\n",
    "\n",
    "#데이터 프레임 저장\n",
    "now = datetime.datetime.now() \n",
    "news_df.to_csv('//Users//yeijeong//big16//final_project//DATANEWS_DATA//현대로템//{}_{}_page{}_{}.csv'.format(search,now.strftime('%Y%m%d_%H%M%S'),page,maxpage_t),encoding='utf-8-sig',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
