{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import ssl\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "def makeUrl(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to):\n",
    "    urls = []\n",
    "    if page == maxpage_t:       # 최대페이지 1일 경우\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        urls.append(url)\n",
    "    else:\n",
    "        while page <= maxpage_t :\n",
    "            url = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "            page += 10\n",
    "    return urls    \n",
    "\n",
    "# html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "def news_attrs_crawler(articles,attrs):\n",
    "    attrs_content=[]\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "\n",
    "#html생성해서 기사크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    #html 불러오기\n",
    "    original_html = requests.get(url, headers=headers, verify=False)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "\n",
    "    url_naver = html.select(\"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver,'href')\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6045/6045 [00:00<00:00, 1945711.59it/s]\n"
     ]
    }
   ],
   "source": [
    "#####뉴스크롤링 시작#####\n",
    "\n",
    "# 최대 페이지 수\n",
    "maxpage = int(input(\"최대 크롤링할 페이지 수 입력하시오: \"))\n",
    "#검색어 입력\n",
    "search = input(\"검색할 키워드를 입력해주세요:\")\n",
    "# 검색 방식\n",
    "sort = input(\"뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): \")    #관련도순=0  최신순=1  오래된순=2\n",
    "# 시작날짜\n",
    "s_date = input(\"시작날짜 입력(2019.01.04):\")  #2019.01.04\n",
    "# 끝나는 날짜\n",
    "e_date = input(\"끝날짜 입력(2019.01.05):\")   #2019.01.05\n",
    "\n",
    "# 날짜 바꾸기\n",
    "s_from = s_date.replace(\".\",\"\")\n",
    "e_to = e_date.replace(\".\",\"\")\n",
    "\n",
    "page = 1  \n",
    "maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "# naver url 생성\n",
    "urls = makeUrl(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "\n",
    "#뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_url =[]\n",
    "news_contents =[]\n",
    "news_dates = []\n",
    "\n",
    "# for i in urls:\n",
    "#     url = articles_crawler(urls)\n",
    "#     news_url.append(url)\n",
    "\n",
    "while page <= maxpage_t :\n",
    "    url_origin = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "    url = articles_crawler(url_origin)\n",
    "    news_url .append(url)\n",
    "    page += 10\n",
    "\n",
    "#제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "def makeList(newlist, content):\n",
    "    for i in content:\n",
    "        for j in i:\n",
    "            newlist.append(j)\n",
    "    return newlist\n",
    "\n",
    "#제목, 링크, 내용 담을 리스트 생성\n",
    "news_url_1 = []\n",
    "\n",
    "#1차원 리스트로 만들기(내용 제외)\n",
    "makeList(news_url_1,news_url)\n",
    "\n",
    "#NAVER 뉴스만 남기기\n",
    "final_urls = []\n",
    "for i in tqdm(range(len(news_url_1))):\n",
    "    if \"news.naver.com\" in news_url_1[i]:\n",
    "        final_urls.append(news_url_1[i])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2035/2035 [15:39<00:00,  2.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 내용 크롤링\n",
    "\n",
    "for i in tqdm(final_urls):\n",
    "    #각 기사 html get하기\n",
    "    headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "    news = requests.get(i, headers=headers, verify=False)\n",
    "    news_html = BeautifulSoup(news.text,\"html.parser\")\n",
    "\n",
    "    # 뉴스 제목 가져오기\n",
    "    title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    if title == None:\n",
    "        title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "    \n",
    "    # 뉴스 본문 가져오기\n",
    "    content = news_html.select(\"div#dic_area\")\n",
    "    if content == []:\n",
    "        content = news_html.select(\"#articeBody\")\n",
    "        \n",
    "    # 기사 텍스트만 가져오기\n",
    "    # list합치기\n",
    "    content = ''.join(str(content))\n",
    "\n",
    "    # html태그제거 및 텍스트 다듬기\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "    content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "    pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "    content = content.replace(pattern2, '')\n",
    "\n",
    "    news_titles.append(title)\n",
    "    news_contents.append(content)\n",
    "\n",
    "    # html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "    # news_date = html_date.attrs['data-date-time']\n",
    "    # news_dates.append(news_date)\n",
    "\n",
    "    try:\n",
    "        html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "        news_date = html_date.attrs['data-date-time']\n",
    "    except AttributeError:\n",
    "        news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "        news_date = re.sub(pattern=pattern1,repl='',string=str(news_date))\n",
    "    # 날짜 가져오기\n",
    "    news_dates.append(news_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#데이터 프레임 만들기\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m news_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mdate\u001b[39;49m\u001b[39m'\u001b[39;49m:news_dates,\u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m:news_titles,\u001b[39m'\u001b[39;49m\u001b[39mlink\u001b[39;49m\u001b[39m'\u001b[39;49m:final_urls,\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m:news_contents})\n\u001b[1;32m      6\u001b[0m news_df\n\u001b[1;32m      8\u001b[0m \u001b[39m#중복 행 지우기\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/trading/lib/python3.10/site-packages/pandas/core/frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    703\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    704\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    705\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    708\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    710\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    711\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/miniforge3/envs/trading/lib/python3.10/site-packages/pandas/core/internals/construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    479\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/miniforge3/envs/trading/lib/python3.10/site-packages/pandas/core/internals/construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    113\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    116\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/miniforge3/envs/trading/lib/python3.10/site-packages/pandas/core/internals/construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    653\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    657\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    658\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    659\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "###데이터 프레임으로 만들기###\n",
    "import pandas as pd\n",
    "\n",
    "#데이터 프레임 만들기\n",
    "news_df = pd.DataFrame({'date':news_dates,'title':news_titles,'link':final_urls,'content':news_contents})\n",
    "news_df\n",
    "\n",
    "#중복 행 지우기\n",
    "news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "print(\"중복 제거 후 행 개수: \",len(news_df))\n",
    "\n",
    "#데이터 프레임 저장\n",
    "now = datetime.now() \n",
    "news_df.to_csv('//Users//yeijeong//big16//final_project//DATA//PAST_NEWS_DATA//{}_{}_date{}_{}.csv'.format(search,now.strftime('%Y%m%d_%H%M%S'),s_from, e_to),encoding='utf-8-sig',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
