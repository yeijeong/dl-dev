{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sections : '정치', '경제', '사회'\n",
    "- sections = [\"pol\", \"eco\",\"soc\"]\n",
    "- section_ids = [\"100\", \"101\",\"102\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4.element\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame(\n",
    "    data=None\n",
    "    , index=None\n",
    "    , columns=['date','title','content']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConnectionError방지\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "\n",
    "# BeautifulSoup 객체 생성\n",
    "def get_soup_obj(url):\n",
    "    res = requests.get(url, headers = headers, verify=False)\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스의 기본 정보 가져오기\n",
    "def get_top3_news_info():\n",
    "    news_urls =[]\n",
    "    \n",
    "    # 해당 분야 상위 뉴스 목록 주소\n",
    "    sec_url = \"https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=001\"\n",
    "    \n",
    "    # 해당 분야 상위 뉴스 HTML 가져오기\n",
    "    soup = get_soup_obj(sec_url)\n",
    "  \n",
    "    # 해당 분야 상위 뉴스 3개 가져오기\n",
    "\n",
    "    lis3 = soup.find('ul', class_='type06_headline').find_all(\"li\", limit=3)\n",
    "    for li in lis3:\n",
    "        news_url = li.a.attrs.get('href')\n",
    "        news_urls.append(news_url)\n",
    "        \n",
    "    return news_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 3개 뉴스 크롤링\n",
    "def F_crawling(news_urls) :\n",
    "    news_titles = []\n",
    "    news_contents =[]\n",
    "    news_dates = []\n",
    "\n",
    "    for url in news_urls:\n",
    "        news_html = get_soup_obj(url)\n",
    "\n",
    "        # 뉴스 제목 가져오기\n",
    "        title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "        if title == None:\n",
    "            title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "\n",
    "        # 뉴스 본문 가져오기\n",
    "        content = news_html.select(\"div#dic_area\")\n",
    "        if content == []:\n",
    "            content = news_html.select(\"#articeBody\")\n",
    "            \n",
    "        # 기사 텍스트만 가져오기\n",
    "        # list합치기\n",
    "        content = ''.join(str(content))\n",
    "\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        pattern1 = '<[^>]*>'\n",
    "        title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "        content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = content.replace(pattern2, '')\n",
    "\n",
    "        news_titles.append(title)\n",
    "        news_contents.append(content)\n",
    "\n",
    "        try:\n",
    "            html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "            news_date = html_date.attrs['data-date-time']\n",
    "        except AttributeError:\n",
    "            news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "            news_date = re.sub(pattern=pattern1,repl='',string=str(news_date))\n",
    "        # 날짜 가져오기\n",
    "        news_dates.append(news_date)\n",
    "\n",
    "        # html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "        # news_date = html_date.attrs['data-date-time']\n",
    "        # news_dates.append(news_date)\n",
    "    \n",
    "    a = pd.DataFrame({'date':news_dates,'title':news_titles,'content':news_contents})\n",
    "\n",
    "    #중복 행 지우기\n",
    "    a = a.drop_duplicates(keep='first',ignore_index=True)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(d):\n",
    "  text = re.sub(r'\\([^)]*\\)', '', str(d))\n",
    "  text = re.sub(r'\\[[^]]*\\]', '', text)\n",
    "  text = re.sub(r'\\<[^>]*\\>', '', text)\n",
    "  pattern = r'[^가-힣0-9a-zA-Z\\s]'\n",
    "  text = re.sub(pattern, ' ', text)\n",
    "  text = re.sub(r'사진', ' ', text)\n",
    "  text = re.sub(r'.*뉴스', ' ', text)\n",
    "  text = re.sub(\"\\n\", ' ', text)\n",
    "  text = re.sub(\"  +\", \" \", text)\n",
    "  return text\n",
    "\n",
    "def text_clean(text):\n",
    "  pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '[a-zA-Z0-9]'    # 숫자와 알파벳 제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '[\\n]'            # \\n제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '[\\t]'            # \\n제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '[\\']'           \n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '[\\\"]'            \n",
    "  text = re.sub(pattern, '', text)\n",
    "  return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dbeaver(NewsDate, NewsTitle, NewsContent) :\n",
    "    conn = pymysql.connect(\n",
    "        host='34.64.240.96'\n",
    "        , user='root'\n",
    "        , password='tndusWkd1.'\n",
    "        , db='final_project'\n",
    "        , charset='utf8'\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    for date,title,content in zip(NewsDate, NewsTitle, NewsContent):\n",
    "        sql = \"INSERT INTO past_news_realtime VALUES ({}, {}, {})\".format(\"\\\"\"+date+\"\\\"\", \"\\\"\"+title+\"\\\"\", \"\\\"\"+content+\"\\\"\")\n",
    "        # print(sql)\n",
    "        try :\n",
    "            cur.execute(sql)\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "        except :\n",
    "            print(\"에러\")\n",
    "            return\n",
    "    # conn.commit()\n",
    "    # conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무한루프 크롤링\n",
    "while True :\n",
    "    news_urls = get_top3_news_info()\n",
    "    a = F_crawling(news_urls)\n",
    "    df_all = pd.merge(a, news_df, how='outer', indicator=True)\n",
    "    news_df = pd.concat([news_df, a], ignore_index=True, keys=['date','title','content'])\n",
    "    news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "    str_expr = '_merge == \\\"left_only\\\"'\n",
    "    df_all2 = df_all.query(str_expr)\n",
    "    df_all2.drop(columns=['_merge'], inplace=True)\n",
    "\n",
    "    df_all2.dropna(axis=0, inplace=True)\n",
    "\n",
    "    df_all2['content'] = df_all2['content'].apply(text_clean)\n",
    "    df_all2['content'] = df_all2['content'].apply(clean_text)\n",
    "    df_all2['title'] = df_all2['title'].apply(text_clean)\n",
    "    df_all2['title'] = df_all2['title'].apply(clean_text)\n",
    "\n",
    "\n",
    "    df_all2.dropna(axis=0, inplace=True)\n",
    "\n",
    "    day = datetime.now()\n",
    "    filepath = 'D:\\\\big16\\\\final project\\\\DATA\\\\REAL_NEWS_DATA\\\\real_time_news_{}.csv'.format(day.strftime('%Y%m%d'))\n",
    "    if not os.path.exists(filepath):\n",
    "        df_all2.to_csv(filepath, index=False, mode='w', encoding='utf-8-sig')\n",
    "    else:\n",
    "        df_all2.to_csv(filepath, index=False, mode='a', encoding='utf-8-sig', header=False) \n",
    "\n",
    "    NewsDate = df_all2['date'].to_list()\n",
    "    NewsTitle = df_all2['title'].to_list()\n",
    "    NewsContent = df_all2['content'].to_list()\n",
    "\n",
    "    to_dbeaver(NewsDate, NewsTitle, NewsContent)\n",
    "                      \n",
    "    time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
