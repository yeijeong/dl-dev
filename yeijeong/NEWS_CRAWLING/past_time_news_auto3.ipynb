{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import ssl\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "def makeUrl(search, page, maxpage_t, sort, s_date, e_date, s_from, e_to):\n",
    "    urls = []\n",
    "    if page == maxpage_t:       # 최대페이지 1일 경우\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        urls.append(url)\n",
    "    else:\n",
    "        while page <= maxpage_t :\n",
    "            url = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "            page += 10\n",
    "    return urls    \n",
    "\n",
    "# html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "def news_attrs_crawler(articles,attrs):\n",
    "    attrs_content=[]\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "\n",
    "#html생성해서 기사크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    #html 불러오기\n",
    "    original_html = requests.get(url, headers=headers, verify=False)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "\n",
    "    url_naver = html.select(\"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver,'href')\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_all(search, page, maxpage_t, sort, s_date, e_date, s_from, e_to) :\n",
    "    # naver url 생성\n",
    "    urls = makeUrl(search, page, maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "\n",
    "    #뉴스 크롤러 실행\n",
    "    news_titles = []\n",
    "    news_url =[]\n",
    "    news_contents =[]\n",
    "    news_dates = []\n",
    "\n",
    "    while page <= maxpage_t :\n",
    "        url_origin = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        url = articles_crawler(url_origin)\n",
    "        news_url .append(url)\n",
    "        page += 10\n",
    "\n",
    "    #제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "    def makeList(newlist, content):\n",
    "        for i in content:\n",
    "            for j in i:\n",
    "                newlist.append(j)\n",
    "        return newlist\n",
    "\n",
    "    #제목, 링크, 내용 담을 리스트 생성\n",
    "    news_url_1 = []\n",
    "\n",
    "    #1차원 리스트로 만들기(내용 제외)\n",
    "    makeList(news_url_1,news_url)\n",
    "\n",
    "    #NAVER 뉴스만 남기기\n",
    "    final_urls = []\n",
    "    for i in tqdm(range(len(news_url_1))):\n",
    "        if \"news.naver.com\" in news_url_1[i]:\n",
    "            final_urls.append(news_url_1[i])\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    for i in tqdm(final_urls):\n",
    "        #각 기사 html get하기\n",
    "        headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "        news = requests.get(i, headers=headers, verify=False)\n",
    "        news_html = BeautifulSoup(news.text,\"html.parser\")\n",
    "\n",
    "        # 뉴스 제목 가져오기\n",
    "        title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "        if title == None:\n",
    "            title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "        \n",
    "        # 뉴스 본문 가져오기\n",
    "        content = news_html.select(\"div#dic_area\")\n",
    "        if content == []:\n",
    "            content = news_html.select(\"#articeBody\")\n",
    "            \n",
    "        # 기사 텍스트만 가져오기\n",
    "        # list합치기\n",
    "        content = ''.join(str(content))\n",
    "\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        pattern1 = '<[^>]*>'\n",
    "        title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "        content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = content.replace(pattern2, '')\n",
    "\n",
    "        news_titles.append(title)\n",
    "        news_contents.append(content)\n",
    "\n",
    "        # html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "        # news_date = html_date.attrs['data-date-time']\n",
    "        # news_dates.append(news_date)\n",
    "\n",
    "        try:\n",
    "            html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "            news_date = html_date.attrs['data-date-time']\n",
    "        except AttributeError:\n",
    "            news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "            news_date = re.sub(pattern=pattern1,repl='',string=str(news_date))\n",
    "        # 날짜 가져오기\n",
    "        news_dates.append(news_date)\n",
    "\n",
    "    a = pd.DataFrame({'date':news_dates,'title':news_titles,'link':final_urls,'content':news_contents})\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          삼성전자\n",
       "1      LG에너지솔루션\n",
       "2        SK하이닉스\n",
       "3      삼성바이오로직스\n",
       "4         삼성SDI\n",
       "         ...   \n",
       "195         지누스\n",
       "196       쿠쿠홈시스\n",
       "197    신세계인터내셔날\n",
       "198       현대홈쇼핑\n",
       "199          한섬\n",
       "Name: 종목명, Length: 200, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kospi = pd.read_csv('D:\\\\big16\\\\final project\\\\DATA\\\\KOSPI200.csv', encoding='euc-kr')\n",
    "kospi200 = kospi['종목명']\n",
    "kospi200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021.07.01', '2021.08.01', '2021.09.01', '2021.10.01', '2021.11.01', '2021.12.01']\n"
     ]
    }
   ],
   "source": [
    "day = []\n",
    "i = 7\n",
    "while i<13:\n",
    "    day.append('2021.{}.01'.format(str(i).zfill(2)))\n",
    "    i += 1\n",
    "\n",
    "print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021.07.01', '2021.08.01', '2021.09.01', '2021.10.01', '2021.11.01', '2021.12.01', '2022.01.01', '2022.02.01', '2022.03.01', '2022.04.01', '2022.05.01', '2022.06.01', '2022.07.01', '2022.08.01', '2022.09.01', '2022.10.01', '2022.11.01', '2022.12.01']\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while i<13:\n",
    "    day.append('2022.{}.01'.format(str(i).zfill(2)))\n",
    "    i += 1\n",
    "\n",
    "print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021.07.01', '2021.08.01', '2021.09.01', '2021.10.01', '2021.11.01', '2021.12.01', '2022.01.01', '2022.02.01', '2022.03.01', '2022.04.01', '2022.05.01', '2022.06.01', '2022.07.01', '2022.08.01', '2022.09.01', '2022.10.01', '2022.11.01', '2022.12.01', '2023.01.01', '2023.02.01', '2023.03.01', '2023.04.01', '2023.05.01', '2023.06.01']\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while i<7:\n",
    "    day.append('2023.{}.01'.format(str(i).zfill(2)))\n",
    "    i += 1\n",
    "\n",
    "print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in kospi200 : \n",
    "    #데이터 프레임 만들기\n",
    "    news_df = news_df = pd.DataFrame(\n",
    "        data=None\n",
    "        , index=None\n",
    "        , columns=['date','title','link''content']\n",
    "    )\n",
    "\n",
    "    # 최대 페이지 수\n",
    "    maxpage = 401\n",
    "    #검색어 입력\n",
    "    search = keyword\n",
    "    # 검색 방식\n",
    "    sort = '0'\n",
    "    # 시작날짜\n",
    "    s_date = '2021.06.28'\n",
    "    # 끝나는 날짜\n",
    "    e_date = '2021.06.30'\n",
    "\n",
    "    # 날짜 바꾸기\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "\n",
    "    page = 1  \n",
    "    maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "    a = crawling_all(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "    news_df = pd.concat([news_df, a], ignore_index=True, keys=['date','title','link''content'])\n",
    "    news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "\n",
    "    now = datetime.now()\n",
    "    news_df.to_csv('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}_{}_date{}_{}.csv'.format(search,now.strftime('%Y%m%d_%H%M%S'),s_from, e_to),encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6661/6661 [00:00<00:00, 1669650.33it/s]\n",
      " 61%|██████▏   | 1630/2651 [08:13<05:09,  3.30it/s]\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "(\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\urllib3\\response.py:761\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 761\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(line, \u001b[39m16\u001b[39;49m)\n\u001b[0;32m    762\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    763\u001b[0m     \u001b[39m# Invalid chunked protocol response, abort.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 16: b''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidChunkLength\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\urllib3\\response.py:444\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    447\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\urllib3\\response.py:828\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 828\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_chunk_length()\n\u001b[0;32m    829\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\urllib3\\response.py:765\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[1;32m--> 765\u001b[0m \u001b[39mraise\u001b[39;00m InvalidChunkLength(\u001b[39mself\u001b[39m, line)\n",
      "\u001b[1;31mInvalidChunkLength\u001b[0m: InvalidChunkLength(got length b'', 0 bytes read)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\urllib3\\response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 624\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content):\n\u001b[0;32m    625\u001b[0m         \u001b[39myield\u001b[39;00m line\n",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\urllib3\\response.py:816\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    811\u001b[0m     \u001b[39mraise\u001b[39;00m BodyNotHttplibCompatible(\n\u001b[0;32m    812\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBody should be http.client.HTTPResponse like. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt should have have an fp attribute which returns raw chunks.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    814\u001b[0m     )\n\u001b[1;32m--> 816\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m    817\u001b[0m     \u001b[39m# Don't bother reading the body of a HEAD request.\u001b[39;00m\n\u001b[0;32m    818\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_response \u001b[39mand\u001b[39;00m is_response_to_head(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_response):\n",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[0;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\urllib3\\response.py:461\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mexcept\u001b[39;00m (HTTPException, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    460\u001b[0m     \u001b[39m# This includes IncompleteRead.\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m     \u001b[39mraise\u001b[39;00m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection broken: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e, e)\n\u001b[0;32m    463\u001b[0m \u001b[39m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[39m# unnecessarily.\u001b[39;00m\n",
      "\u001b[1;31mProtocolError\u001b[0m: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m page \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \n\u001b[0;32m     29\u001b[0m maxpage_t \u001b[39m=\u001b[39m(\u001b[39mint\u001b[39m(maxpage)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m   \u001b[39m# 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m a \u001b[39m=\u001b[39m crawling_all(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to)\n\u001b[0;32m     32\u001b[0m news_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([news_df, a], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, keys\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     33\u001b[0m news_df \u001b[39m=\u001b[39m news_df\u001b[39m.\u001b[39mdrop_duplicates(keep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m,ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[13], line 41\u001b[0m, in \u001b[0;36mcrawling_all\u001b[1;34m(search, page, maxpage_t, sort, s_date, e_date, s_from, e_to)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(final_urls):\n\u001b[0;32m     39\u001b[0m     \u001b[39m#각 기사 html get하기\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     headers \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m'\u001b[39m\u001b[39mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m---> 41\u001b[0m     news \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(i, headers\u001b[39m=\u001b[39;49mheaders, verify\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     42\u001b[0m     news_html \u001b[39m=\u001b[39m BeautifulSoup(news\u001b[39m.\u001b[39mtext,\u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m     \u001b[39m# 뉴스 제목 가져오기\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     r\u001b[39m.\u001b[39;49mcontent\n\u001b[0;32m    749\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tj\\anaconda3\\envs\\trading\\lib\\site-packages\\requests\\models.py:818\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 818\u001b[0m     \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[0;32m    819\u001b[0m \u001b[39mexcept\u001b[39;00m DecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    820\u001b[0m     \u001b[39mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))"
     ]
    }
   ],
   "source": [
    "#####뉴스크롤링 시작#####\n",
    "for keyword in kospi200 : \n",
    "    for i in day :\n",
    "        #데이터 프레임 만들기\n",
    "        news_df = news_df = pd.DataFrame(\n",
    "            data=None\n",
    "            , index=None\n",
    "            , columns=['date','title','link''content']\n",
    "        )\n",
    "\n",
    "        # 최대 페이지 수\n",
    "        maxpage = 401\n",
    "        #검색어 입력\n",
    "        search = keyword\n",
    "        # 검색 방식\n",
    "        sort = str(0)\n",
    "        # 시작날짜\n",
    "        s_date = i\n",
    "        # 끝나는 날짜\n",
    "        e_1 = i[-2:]\n",
    "        e_2 = int(e_1)+30\n",
    "        e_3 = i[:-2]+str(e_2)\n",
    "        e_date = e_3\n",
    "\n",
    "        # 날짜 바꾸기\n",
    "        s_from = s_date.replace(\".\",\"\")\n",
    "        e_to = e_date.replace(\".\",\"\")\n",
    "\n",
    "        page = 1  \n",
    "        maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "        a = crawling_all(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "        news_df = pd.concat([news_df, a], ignore_index=True, keys=['date','title','link''content'])\n",
    "        news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "\n",
    "        now = datetime.now()\n",
    "        news_df.to_csv('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}_{}_date{}_{}.csv'.format(search,now.strftime('%Y%m%d_%H%M%S'),s_from, e_to),encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in kospi200 : \n",
    "    #데이터 프레임 만들기\n",
    "    news_df = news_df = pd.DataFrame(\n",
    "        data=None\n",
    "        , index=None\n",
    "        , columns=['date','title','link''content']\n",
    "    )\n",
    "\n",
    "    # 최대 페이지 수\n",
    "    maxpage = 401\n",
    "    #검색어 입력\n",
    "    search = keyword\n",
    "    # 검색 방식\n",
    "    sort = '0'\n",
    "    # 시작날짜\n",
    "    s_date = '2023.07.01'\n",
    "    # 끝나는 날짜\n",
    "    e_date = '2023.07.07'\n",
    "\n",
    "    # 날짜 바꾸기\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "\n",
    "    page = 1  \n",
    "    maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "    a = crawling_all(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "    news_df = pd.concat([news_df, a], ignore_index=True, keys=['date','title','link''content'])\n",
    "    news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "\n",
    "    now = datetime.now()\n",
    "    news_df.to_csv('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}_{}_date{}_{}.csv'.format(search,now.strftime('%Y%m%d_%H%M%S'),s_from, e_to),encoding='utf-8-sig',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
