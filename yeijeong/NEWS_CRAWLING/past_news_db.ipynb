{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import ssl\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import numpy as np\n",
    "import bs4.element\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "def makeUrl(search, page, maxpage_t, sort, s_date, e_date, s_from, e_to):\n",
    "    urls = []\n",
    "    if page == maxpage_t:       # 최대페이지 1일 경우\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        urls.append(url)\n",
    "    else:\n",
    "        while page <= maxpage_t :\n",
    "            url = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "            page += 10\n",
    "    return urls    \n",
    "\n",
    "# html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "def news_attrs_crawler(articles,attrs):\n",
    "    attrs_content=[]\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "\n",
    "#html생성해서 기사크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    #html 불러오기\n",
    "    original_html = requests.get(url, headers=headers, verify=False)\n",
    "\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "\n",
    "    url_naver = html.select(\"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver,'href')\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_all(search, page, maxpage_t, sort, s_date, e_date, s_from, e_to) :\n",
    "    # naver url 생성\n",
    "    urls = makeUrl(search, page, maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "\n",
    "    #뉴스 크롤러 실행\n",
    "    news_titles = []\n",
    "    news_url =[]\n",
    "    news_contents =[]\n",
    "    news_dates = []\n",
    "\n",
    "    while page <= maxpage_t :\n",
    "        url_origin = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        url = articles_crawler(url_origin)\n",
    "        news_url .append(url)\n",
    "        page += 10\n",
    "\n",
    "    #제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "    def makeList(newlist, content):\n",
    "        for i in content:\n",
    "            for j in i:\n",
    "                newlist.append(j)\n",
    "        return newlist\n",
    "\n",
    "    #제목, 링크, 내용 담을 리스트 생성\n",
    "    news_url_1 = []\n",
    "\n",
    "    #1차원 리스트로 만들기(내용 제외)\n",
    "    makeList(news_url_1,news_url)\n",
    "\n",
    "    #NAVER 뉴스만 남기기\n",
    "    final_urls = []\n",
    "    for i in range(len(news_url_1)):\n",
    "        if \"news.naver.com\" in news_url_1[i]:\n",
    "            final_urls.append(news_url_1[i])\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    for i in final_urls:\n",
    "        #각 기사 html get하기\n",
    "        headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "        news = requests.get(i, headers=headers, verify=False)\n",
    "        news_html = BeautifulSoup(news.text,\"html.parser\")\n",
    "\n",
    "        # 뉴스 제목 가져오기\n",
    "        title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "        if title == None:\n",
    "            title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "        \n",
    "        # 뉴스 본문 가져오기\n",
    "        content = news_html.select(\"div#dic_area\")\n",
    "        if content == []:\n",
    "            content = news_html.select(\"#articeBody\")\n",
    "            \n",
    "        # 기사 텍스트만 가져오기\n",
    "        # list합치기\n",
    "        content = ''.join(str(content))\n",
    "\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        pattern1 = '<[^>]*>'\n",
    "        title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "        content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = content.replace(pattern2, '')\n",
    "\n",
    "        news_titles.append(title)\n",
    "        news_contents.append(content)\n",
    "\n",
    "        # html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "        # news_date = html_date.attrs['data-date-time']\n",
    "        # news_dates.append(news_date)\n",
    "\n",
    "        try:\n",
    "            html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "            news_date = html_date.attrs['data-date-time']\n",
    "        except AttributeError:\n",
    "            news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "            news_date = re.sub(pattern=pattern1,repl='',string=str(news_date))\n",
    "        # 날짜 가져오기\n",
    "        news_dates.append(news_date)\n",
    "\n",
    "    a = pd.DataFrame({'date':news_dates,'title':news_titles,'link':final_urls,'content':news_contents})\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kospi = pd.read_csv('D:\\\\big16\\\\final project\\\\DATA\\\\KOSPI200.csv', encoding='euc-kr')\n",
    "# kospi200 = kospi['종목명']\n",
    "# kospi200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021.07.01', '2021.08.01', '2021.09.01', '2021.10.01', '2021.11.01', '2021.12.01']\n"
     ]
    }
   ],
   "source": [
    "day = []\n",
    "i = 7\n",
    "while i<13:\n",
    "    day.append('2021.{}.01'.format(str(i).zfill(2)))\n",
    "    i += 1\n",
    "\n",
    "print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021.07.01', '2021.08.01', '2021.09.01', '2021.10.01', '2021.11.01', '2021.12.01', '2022.01.01', '2022.02.01', '2022.03.01', '2022.04.01', '2022.05.01', '2022.06.01', '2022.07.01', '2022.08.01', '2022.09.01', '2022.10.01', '2022.11.01', '2022.12.01']\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while i<13:\n",
    "    day.append('2022.{}.01'.format(str(i).zfill(2)))\n",
    "    i += 1\n",
    "\n",
    "print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021.07.01', '2021.08.01', '2021.09.01', '2021.10.01', '2021.11.01', '2021.12.01', '2022.01.01', '2022.02.01', '2022.03.01', '2022.04.01', '2022.05.01', '2022.06.01', '2022.07.01', '2022.08.01', '2022.09.01', '2022.10.01', '2022.11.01', '2022.12.01', '2023.01.01', '2023.02.01', '2023.03.01', '2023.04.01', '2023.05.01', '2023.06.01']\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while i<7:\n",
    "    day.append('2023.{}.01'.format(str(i).zfill(2)))\n",
    "    i += 1\n",
    "\n",
    "print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 프레임 만들기\n",
    "news_df = news_df = pd.DataFrame(\n",
    "    data=None\n",
    "    , index=None\n",
    "    , columns=['date','title','link','content']\n",
    ")\n",
    "\n",
    "# 최대 페이지 수\n",
    "maxpage = 401\n",
    "#검색어 입력\n",
    "search = '미국'\n",
    "# 검색 방식\n",
    "sort = '0'\n",
    "# 시작날짜\n",
    "s_date = '2021.06.28'\n",
    "# 끝나는 날짜\n",
    "e_date = '2021.06.30'\n",
    "\n",
    "# 날짜 바꾸기\n",
    "s_from = s_date.replace(\".\",\"\")\n",
    "e_to = e_date.replace(\".\",\"\")\n",
    "\n",
    "page = 1  \n",
    "maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "a = crawling_all(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "news_df = pd.concat([news_df, a], ignore_index=True, keys=['date','title','link','content'])\n",
    "news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "\n",
    "now = datetime.now()\n",
    "news_df.to_csv('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}\\\\{}_date{}_{}.csv'.format(search, search,s_from, e_to),encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [36:51<00:00, 92.13s/it]  \n"
     ]
    }
   ],
   "source": [
    "#####뉴스크롤링 시작#####\n",
    "for i in tqdm(day) :\n",
    "    #데이터 프레임 만들기\n",
    "    news_df = news_df = pd.DataFrame(\n",
    "        data=None\n",
    "        , index=None\n",
    "        , columns=['date','title','link','content']\n",
    "    )\n",
    "\n",
    "    # 최대 페이지 수\n",
    "    maxpage = 401\n",
    "    #검색어 입력\n",
    "    search = '미국'\n",
    "    # 검색 방식\n",
    "    sort = str(0)\n",
    "    # 시작날짜\n",
    "    s_date = i\n",
    "    # 끝나는 날짜\n",
    "    e_1 = i[-2:]\n",
    "    e_2 = int(e_1)+30\n",
    "    e_3 = i[:-2]+str(e_2)\n",
    "    e_date = e_3\n",
    "\n",
    "    # 날짜 바꾸기\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "\n",
    "    page = 1  \n",
    "    maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "    a = crawling_all(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "    news_df = pd.concat([news_df, a], ignore_index=True, keys=['date','title','link','content'])\n",
    "    news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "\n",
    "    now = datetime.now()\n",
    "    news_df.to_csv('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}\\\\{}_date{}_{}.csv'.format(search, search,s_from, e_to),encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 프레임 만들기\n",
    "news_df = news_df = pd.DataFrame(\n",
    "    data=None\n",
    "    , index=None\n",
    "    , columns=['date','title','link','content']\n",
    ")\n",
    "\n",
    "# 최대 페이지 수\n",
    "maxpage = 401\n",
    "#검색어 입력\n",
    "search = '미국'\n",
    "# 검색 방식\n",
    "sort = '0'\n",
    "# 시작날짜\n",
    "s_date = '2023.07.01'\n",
    "# 끝나는 날짜\n",
    "e_date = '2023.07.11'\n",
    "\n",
    "# 날짜 바꾸기\n",
    "s_from = s_date.replace(\".\",\"\")\n",
    "e_to = e_date.replace(\".\",\"\")\n",
    "\n",
    "page = 1  \n",
    "maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "a = crawling_all(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "news_df = pd.concat([news_df, a], ignore_index=True, keys=['date','title','link','content'])\n",
    "news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "\n",
    "now = datetime.now()\n",
    "news_df.to_csv('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}\\\\{}_date{}_{}.csv'.format(search, search,s_from, e_to),encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}'.format(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "for i in range(0,len(folders)):\n",
    "    if folders[i].split('.')[1] == 'csv':\n",
    "        file = 'D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}\\\\'.format(search)+folders[i]\n",
    "        df= pd.read_csv(file,encoding='utf-8') \n",
    "        df_all = pd.concat([df_all, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>linkcontent</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-30 16:00:02</td>\n",
       "      <td>실수로 입금된 56조원…나흘간 세계 25위 부자된 미국 부부</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/001/001...</td>\n",
       "      <td>[\\n은행 \"기술 오류로 고객 계좌 영향\" 즉각 신고했지만 나흘 뒤에야 회수\"계좌에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-30 22:34:04</td>\n",
       "      <td>\"소련은 해방군, 미국은 점령군\"...김원웅 광복회장, 고교생 대상 강연 논란</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/029/000...</td>\n",
       "      <td>[\\n\\n\\n\\n\\n광복회 관련 보도에 대한 입장 발표하는 김원웅 광복회장. [연합...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-30 14:59:03</td>\n",
       "      <td>미국 집값 4월 14.6% 폭등…왜 이렇게 오르나</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/032/000...</td>\n",
       "      <td>[\\n\\t\\t\\t[경향신문] 미국에서도 주택시장 과열이 심각해지고 있다. 올 4월 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-30 22:13:01</td>\n",
       "      <td>\"델타 델타플러스 무섭게 퍼진다\"…미국 러시아 한국 전세계가 초비상</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "      <td>[\\n\\n\\n\\n\\n[사진 제공 = 연합뉴스] 코로나19 바이러스에서 파생한 델타,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-30 07:35:18</td>\n",
       "      <td>\"너무 이뻐 눈을 못떼겠네\" 트렌스젠더 여성, 미국 미인대회 우승</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/014/000...</td>\n",
       "      <td>[\\n\\n\\n\\n\\n카탈루나 엔리케스. 카탈루나 엔리케스 인스타그램 캡쳐 미국 네바...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                        title  \\\n",
       "0  2021-06-30 16:00:02            실수로 입금된 56조원…나흘간 세계 25위 부자된 미국 부부   \n",
       "1  2021-06-30 22:34:04  \"소련은 해방군, 미국은 점령군\"...김원웅 광복회장, 고교생 대상 강연 논란   \n",
       "2  2021-06-30 14:59:03                  미국 집값 4월 14.6% 폭등…왜 이렇게 오르나   \n",
       "3  2021-06-30 22:13:01        \"델타 델타플러스 무섭게 퍼진다\"…미국 러시아 한국 전세계가 초비상   \n",
       "4  2021-06-30 07:35:18         \"너무 이뻐 눈을 못떼겠네\" 트렌스젠더 여성, 미국 미인대회 우승   \n",
       "\n",
       "  linkcontent                                               link  \\\n",
       "0         NaN  https://n.news.naver.com/mnews/article/001/001...   \n",
       "1         NaN  https://n.news.naver.com/mnews/article/029/000...   \n",
       "2         NaN  https://n.news.naver.com/mnews/article/032/000...   \n",
       "3         NaN  https://n.news.naver.com/mnews/article/009/000...   \n",
       "4         NaN  https://n.news.naver.com/mnews/article/014/000...   \n",
       "\n",
       "                                             content  \n",
       "0  [\\n은행 \"기술 오류로 고객 계좌 영향\" 즉각 신고했지만 나흘 뒤에야 회수\"계좌에...  \n",
       "1  [\\n\\n\\n\\n\\n광복회 관련 보도에 대한 입장 발표하는 김원웅 광복회장. [연합...  \n",
       "2  [\\n\\t\\t\\t[경향신문] 미국에서도 주택시장 과열이 심각해지고 있다. 올 4월 ...  \n",
       "3  [\\n\\n\\n\\n\\n[사진 제공 = 연합뉴스] 코로나19 바이러스에서 파생한 델타,...  \n",
       "4  [\\n\\n\\n\\n\\n카탈루나 엔리케스. 카탈루나 엔리케스 인스타그램 캡쳐 미국 네바...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.drop('link', axis=1, inplace=True)\n",
    "# df_all.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_all.drop('linkcontent', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(d):\n",
    "  text = re.sub(r'\\([^)]*\\)', '', d)\n",
    "  text = re.sub(r'\\[[^]]*\\]', '', text)\n",
    "  text = re.sub(r'\\<[^>]*\\>', '', text)\n",
    "  pattern = r'[^가-힣0-9a-zA-Z\\s]'\n",
    "  text = re.sub(pattern, ' ', text)\n",
    "  text = re.sub(r'사진', ' ', text)\n",
    "  text = re.sub(r'.*뉴스', ' ', text)\n",
    "  text = re.sub(\"\\n\", ' ', text)\n",
    "  text = re.sub(\"  +\", \" \", text)\n",
    "  return text\n",
    "\n",
    "def text_clean(text):\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[a-zA-Z0-9]'    # 숫자와 알파벳 제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[\\n]'            # \\n제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[\\t]'            # \\n제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[\\']'           \n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[\\\"]'            \n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-30 16:00:02</td>\n",
       "      <td>등에 따르면 루이지애나주 배턴루지에서 부동산 중개회사를 운영하는 대런 제임스는 지...</td>\n",
       "      <td>등에 따르면 루이지애나주 배턴루지에서 부동산 중개회사를 운영하는 대런 제임스는 지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-30 22:34:04</td>\n",
       "      <td>김원웅 광복회장이 고등학생을 대상으로 한 영상 강연에서 광복 이후 북한에 진입한 ...</td>\n",
       "      <td>김원웅 광복회장이 고등학생을 대상으로 한 영상 강연에서 광복 이후 북한에 진입한 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-30 14:59:03</td>\n",
       "      <td>경향신문 미국에서도 주택시장 과열이 심각해지고 있다 올 월 미국의 주택가격이 년 만...</td>\n",
       "      <td>경향신문 미국에서도 주택시장 과열이 심각해지고 있다 올 월 미국의 주택가격이 년 만...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-30 22:13:01</td>\n",
       "      <td>코로나 바이러스에서 파생한 델타 델타 플러스 변이 확산으로 전세계가 또다시 고비를...</td>\n",
       "      <td>코로나 바이러스에서 파생한 델타 델타 플러스 변이 확산으로 전세계가 또다시 고비를...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-30 07:35:18</td>\n",
       "      <td>위크 등에 따르면 성전환 여성 모델인 카탈루나 엔리케스는 미스 네바다주 대회에서 ...</td>\n",
       "      <td>위크 등에 따르면 성전환 여성 모델인 카탈루나 엔리케스는 미스 네바다주 대회에서 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-07-10 10:57:03</td>\n",
       "      <td>중국 제외한 전기차 판매량 전년비 증가테슬라 만대 인도하며 위 기록현대차기아 만대 ...</td>\n",
       "      <td>중국 제외한 전기차 판매량 전년비 증가테슬라 만대 인도하며 위 기록현대차기아 만대 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-07-11 16:15:00</td>\n",
       "      <td>무단 전재 및 재배포 금지남아프리카공화국이 세계에서 여행하기 가장 위험한 국가라는...</td>\n",
       "      <td>무단 전재 및 재배포 금지남아프리카공화국이 세계에서 여행하기 가장 위험한 국가라는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-07-11 06:25:04</td>\n",
       "      <td>거리에서 마주치게 돼 정말 반갑고 지난 월 방한 시 환대에 매우 감사드린다고 인사...</td>\n",
       "      <td>거리에서 마주치게 돼 정말 반갑고 지난 월 방한 시 환대에 매우 감사드린다고 인사...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-07-11 10:59:51</td>\n",
       "      <td>반종빈 이재윤 기자 김여정 북한 노동당 부부장은 일 미국 공군 전략정찰기가 동해 ...</td>\n",
       "      <td>반종빈 이재윤 기자 김여정 북한 노동당 부부장은 일 미국 공군 전략정찰기가 동해 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-07-11 07:12:30</td>\n",
       "      <td>는 시간 여러분의 제보를 기다립니다 전화 이메일 카카오톡 제보</td>\n",
       "      <td>는 시간 여러분의 제보를 기다립니다 전화 이메일 카카오톡 제보</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6132 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date                                              title  \\\n",
       "0   2021-06-30 16:00:02   등에 따르면 루이지애나주 배턴루지에서 부동산 중개회사를 운영하는 대런 제임스는 지...   \n",
       "1   2021-06-30 22:34:04   김원웅 광복회장이 고등학생을 대상으로 한 영상 강연에서 광복 이후 북한에 진입한 ...   \n",
       "2   2021-06-30 14:59:03  경향신문 미국에서도 주택시장 과열이 심각해지고 있다 올 월 미국의 주택가격이 년 만...   \n",
       "3   2021-06-30 22:13:01   코로나 바이러스에서 파생한 델타 델타 플러스 변이 확산으로 전세계가 또다시 고비를...   \n",
       "4   2021-06-30 07:35:18   위크 등에 따르면 성전환 여성 모델인 카탈루나 엔리케스는 미스 네바다주 대회에서 ...   \n",
       "..                  ...                                                ...   \n",
       "11  2023-07-10 10:57:03  중국 제외한 전기차 판매량 전년비 증가테슬라 만대 인도하며 위 기록현대차기아 만대 ...   \n",
       "12  2023-07-11 16:15:00   무단 전재 및 재배포 금지남아프리카공화국이 세계에서 여행하기 가장 위험한 국가라는...   \n",
       "13  2023-07-11 06:25:04   거리에서 마주치게 돼 정말 반갑고 지난 월 방한 시 환대에 매우 감사드린다고 인사...   \n",
       "14  2023-07-11 10:59:51   반종빈 이재윤 기자 김여정 북한 노동당 부부장은 일 미국 공군 전략정찰기가 동해 ...   \n",
       "15  2023-07-11 07:12:30                 는 시간 여러분의 제보를 기다립니다 전화 이메일 카카오톡 제보   \n",
       "\n",
       "                                              content  \n",
       "0    등에 따르면 루이지애나주 배턴루지에서 부동산 중개회사를 운영하는 대런 제임스는 지...  \n",
       "1    김원웅 광복회장이 고등학생을 대상으로 한 영상 강연에서 광복 이후 북한에 진입한 ...  \n",
       "2   경향신문 미국에서도 주택시장 과열이 심각해지고 있다 올 월 미국의 주택가격이 년 만...  \n",
       "3    코로나 바이러스에서 파생한 델타 델타 플러스 변이 확산으로 전세계가 또다시 고비를...  \n",
       "4    위크 등에 따르면 성전환 여성 모델인 카탈루나 엔리케스는 미스 네바다주 대회에서 ...  \n",
       "..                                                ...  \n",
       "11  중국 제외한 전기차 판매량 전년비 증가테슬라 만대 인도하며 위 기록현대차기아 만대 ...  \n",
       "12   무단 전재 및 재배포 금지남아프리카공화국이 세계에서 여행하기 가장 위험한 국가라는...  \n",
       "13   거리에서 마주치게 돼 정말 반갑고 지난 월 방한 시 환대에 매우 감사드린다고 인사...  \n",
       "14   반종빈 이재윤 기자 김여정 북한 노동당 부부장은 일 미국 공군 전략정찰기가 동해 ...  \n",
       "15                 는 시간 여러분의 제보를 기다립니다 전화 이메일 카카오톡 제보  \n",
       "\n",
       "[6132 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['content'] = df_all.content.apply(text_clean)\n",
    "df_all['content'] = df_all.content.apply(clean_text)\n",
    "df_all['title'] = df_all.content.apply(text_clean)\n",
    "df_all['title'] = df_all.content.apply(clean_text)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5601 entries, 0 to 15\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   date     5601 non-null   object\n",
      " 1   title    5601 non-null   object\n",
      " 2   content  5601 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 175.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\0. merge\\\\{}_merge.csv'.format(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "conn = pymysql.connect(\n",
    "    host='34.64.240.96'\n",
    "    , user='root'\n",
    "    , password='tndusWkd1.'\n",
    "    , db='final_project'\n",
    "    , charset='utf8'\n",
    ")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewsDate = df_all['date'].to_list()\n",
    "NewsTItle = df_all['title'].to_list()\n",
    "NewsContent = df_all['content'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date,title,content in zip(NewsDate, NewsTItle, NewsContent):\n",
    "    sql = \"INSERT INTO past_news_usa VALUES ({}, {}, {})\".format(\"\\\"\"+date+\"\\\"\", \"\\\"\"+title+\"\\\"\", \"\\\"\"+content+\"\\\"\")\n",
    "    # print(sql)\n",
    "    cur.execute(sql)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
