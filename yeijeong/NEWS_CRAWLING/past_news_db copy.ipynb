{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import ssl\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import numpy as np\n",
    "import bs4.element\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "def makeUrl(search, page, maxpage_t, sort, s_date, e_date, s_from, e_to):\n",
    "    urls = []\n",
    "    if page == maxpage_t:       # 최대페이지 1일 경우\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        urls.append(url)\n",
    "    else:\n",
    "        while page <= maxpage_t :\n",
    "            url = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "            page += 10\n",
    "    return urls    \n",
    "\n",
    "# html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "def news_attrs_crawler(articles,attrs):\n",
    "    attrs_content=[]\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "\n",
    "#html생성해서 기사크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    #html 불러오기\n",
    "    original_html = requests.get(url, headers=headers, verify=False)\n",
    "\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "\n",
    "    url_naver = html.select(\"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver,'href')\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_all(search, page, maxpage_t, sort, s_date, e_date, s_from, e_to) :\n",
    "    # naver url 생성\n",
    "    urls = makeUrl(search, page, maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "\n",
    "    #뉴스 크롤러 실행\n",
    "    news_titles = []\n",
    "    news_url =[]\n",
    "    news_contents =[]\n",
    "    news_dates = []\n",
    "\n",
    "    while page <= maxpage_t :\n",
    "        url_origin = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        url = articles_crawler(url_origin)\n",
    "        news_url .append(url)\n",
    "        page += 10\n",
    "\n",
    "    #제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "    def makeList(newlist, content):\n",
    "        for i in content:\n",
    "            for j in i:\n",
    "                newlist.append(j)\n",
    "        return newlist\n",
    "\n",
    "    #제목, 링크, 내용 담을 리스트 생성\n",
    "    news_url_1 = []\n",
    "\n",
    "    #1차원 리스트로 만들기(내용 제외)\n",
    "    makeList(news_url_1,news_url)\n",
    "\n",
    "    #NAVER 뉴스만 남기기\n",
    "    final_urls = []\n",
    "    for i in range(len(news_url_1)):\n",
    "        if \"news.naver.com\" in news_url_1[i]:\n",
    "            final_urls.append(news_url_1[i])\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    for i in final_urls:\n",
    "        #각 기사 html get하기\n",
    "        headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "        news = requests.get(i, headers=headers, verify=False)\n",
    "        news_html = BeautifulSoup(news.text,\"html.parser\")\n",
    "\n",
    "        # 뉴스 제목 가져오기\n",
    "        title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "        if title == None:\n",
    "            title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "        \n",
    "        # 뉴스 본문 가져오기\n",
    "        content = news_html.select(\"div#dic_area\")\n",
    "        if content == []:\n",
    "            content = news_html.select(\"#articeBody\")\n",
    "            \n",
    "        # 기사 텍스트만 가져오기\n",
    "        # list합치기\n",
    "        content = ''.join(str(content))\n",
    "\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        pattern1 = '<[^>]*>'\n",
    "        title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "        content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = content.replace(pattern2, '')\n",
    "\n",
    "        news_titles.append(title)\n",
    "        news_contents.append(content)\n",
    "\n",
    "        # html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "        # news_date = html_date.attrs['data-date-time']\n",
    "        # news_dates.append(news_date)\n",
    "\n",
    "        try:\n",
    "            html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "            news_date = html_date.attrs['data-date-time']\n",
    "        except AttributeError:\n",
    "            news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "            news_date = re.sub(pattern=pattern1,repl='',string=str(news_date))\n",
    "        # 날짜 가져오기\n",
    "        news_dates.append(news_date)\n",
    "\n",
    "    a = pd.DataFrame({'date':news_dates,'title':news_titles,'content':news_contents})\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(d):\n",
    "  text = re.sub(r'\\([^)]*\\)', '', d)\n",
    "  text = re.sub(r'\\[[^]]*\\]', '', text)\n",
    "  text = re.sub(r'\\<[^>]*\\>', '', text)\n",
    "  pattern = r'[^가-힣0-9a-zA-Z\\s]'\n",
    "  text = re.sub(pattern, ' ', text)\n",
    "  text = re.sub(r'사진', ' ', text)\n",
    "  text = re.sub(r'.*뉴스', ' ', text)\n",
    "  text = re.sub(\"\\n\", ' ', text)\n",
    "  text = re.sub(\"  +\", \" \", text)\n",
    "  return text\n",
    "\n",
    "def text_clean(text):\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[a-zA-Z0-9]'    # 숫자와 알파벳 제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[\\n]'            # \\n제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[\\t]'            # \\n제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[\\']'           \n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '[\\\"]'            \n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword = ['비행체부품', '로켓', '우주기술', '국제시장', '기술 혁신', '연구 개발', '항공우주 정책과 규제', '전투기', '미사일']\n",
    "# basename= ['past_news_aircraftparts', 'past_news_rocket', 'past_news_spacetech', 'past_news_internationalmarket', 'past_news_techrevol', 'past_news_randd', 'past_news_aerospacepolicyr', 'past_news_fighterplane', 'past_news_missile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword = ['전쟁', '무기수출', '미국']\n",
    "# basename = ['past_news_war', 'past_news_armsexport', 'past_news_usa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day = []\n",
    "# i = 1\n",
    "# while i<7:\n",
    "#     day.append('2023.{}.01'.format(str(i).zfill(2)))\n",
    "#     i += 1\n",
    "\n",
    "# print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day = ['2021.08.01', '2021.10.01', '2021.12.01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #데이터 프레임 만들기\n",
    "# for i in tqdm(day) :\n",
    "#     news_df = news_df = pd.DataFrame(\n",
    "#         data=None\n",
    "#         , index=None\n",
    "#         , columns=['date','title','content']\n",
    "#     )\n",
    "\n",
    "#     # 최대 페이지 수\n",
    "#     maxpage = 401\n",
    "#     #검색어 입력\n",
    "#     search = '사회'\n",
    "#     # 검색 방식\n",
    "#     sort = '0'\n",
    "#     # 시작날짜\n",
    "#     s_date = i\n",
    "#     # 끝나는 날짜\n",
    "#     e_1 = i[-2:]\n",
    "#     e_2 = int(e_1)+30\n",
    "#     e_3 = i[:-2]+str(e_2)\n",
    "#     e_date = e_3\n",
    "\n",
    "#     # 날짜 바꾸기\n",
    "#     s_from = s_date.replace(\".\",\"\")\n",
    "#     e_to = e_date.replace(\".\",\"\")\n",
    "\n",
    "#     page = 1  \n",
    "#     maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "#     a = crawling_all(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "#     news_df = pd.concat([news_df, a], ignore_index=True, keys=['date','title','link','content'])\n",
    "#     news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "\n",
    "#     now = datetime.now()\n",
    "#     news_df.to_csv('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}\\\\{}_date{}_{}.csv'.format(search, search,s_from, e_to),encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df = pd.DataFrame(\n",
    "    data=None\n",
    "    , index=None\n",
    "    , columns=['date','title','content']\n",
    ")\n",
    "\n",
    "# 최대 페이지 수\n",
    "maxpage = 401\n",
    "#검색어 입력\n",
    "search = '경제'\n",
    "# 검색 방식\n",
    "sort = '0'\n",
    "# 시작날짜\n",
    "s_date = '2023.07.06'\n",
    "# 끝나는 날짜\n",
    "e_date = '2023.07.15'\n",
    "\n",
    "# 날짜 바꾸기\n",
    "s_from = s_date.replace(\".\",\"\")\n",
    "e_to = e_date.replace(\".\",\"\")\n",
    "\n",
    "page = 1  \n",
    "maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "a = crawling_all(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "news_df = pd.concat([news_df, a], ignore_index=True, keys=['date','title','link','content'])\n",
    "news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "\n",
    "now = datetime.now()\n",
    "news_df.to_csv('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}\\\\{}_date{}_{}.csv'.format(search, search,s_from, e_to),encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_df = news_df = pd.DataFrame(\n",
    "#     data=None\n",
    "#     , index=None\n",
    "#     , columns=['date','title','content']\n",
    "# )\n",
    "\n",
    "# # 최대 페이지 수\n",
    "# maxpage = 401\n",
    "# #검색어 입력\n",
    "# search = '사회'\n",
    "# # 검색 방식\n",
    "# sort = '0'\n",
    "# # 시작날짜\n",
    "# s_date = '2021.11.01'\n",
    "# # 끝나는 날짜\n",
    "# e_date = '2021.11.31'\n",
    "\n",
    "# # 날짜 바꾸기\n",
    "# s_from = s_date.replace(\".\",\"\")\n",
    "# e_to = e_date.replace(\".\",\"\")\n",
    "\n",
    "# page = 1  \n",
    "# maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "# a = crawling_all(search,page,maxpage_t, sort, s_date, e_date, s_from, e_to)\n",
    "# news_df = pd.concat([news_df, a], ignore_index=True, keys=['date','title','link','content'])\n",
    "# news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "\n",
    "# now = datetime.now()\n",
    "# news_df.to_csv('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}\\\\{}_date{}_{}.csv'.format(search, search,s_from, e_to),encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}'.format(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "for i in range(0,len(folders)):\n",
    "    if folders[i].split('.')[1] == 'csv':\n",
    "        file = 'D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\{}\\\\'.format(search)+folders[i]\n",
    "        df= pd.read_csv(file,encoding='utf-8') \n",
    "        df_all = pd.concat([df_all, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['content'] = df_all.content.apply(text_clean)\n",
    "df_all['content'] = df_all.content.apply(clean_text)\n",
    "df_all['title'] = df_all.content.apply(text_clean)\n",
    "df_all['title'] = df_all.content.apply(clean_text)\n",
    "\n",
    "df_all.dropna(axis=0, inplace=True)\n",
    "\n",
    "df_all.to_csv('D:\\\\big16\\\\final project\\\\DATA\\\\NEWS_DATA\\\\0. merge\\\\{}_merge.csv'.format(search), index=False)\n",
    "\n",
    "NewsDate = df_all['date'].to_list()\n",
    "NewsTItle = df_all['title'].to_list()\n",
    "NewsContent = df_all['content'].to_list()\n",
    "\n",
    "import pymysql\n",
    "conn = pymysql.connect(\n",
    "    host='34.64.240.96'\n",
    "    , user='root'\n",
    "    , password='tndusWkd1.'\n",
    "    , db='final_project'\n",
    "    , charset='utf8'\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "for date,title,content in zip(NewsDate, NewsTItle, NewsContent):\n",
    "    sql = \"INSERT INTO past_news_society VALUES ({}, {}, {})\".format(\"\\\"\"+date+\"\\\"\", \"\\\"\"+title+\"\\\"\", \"\\\"\"+content+\"\\\"\")\n",
    "    # print(sql)\n",
    "    cur.execute(sql)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
